k8s$ minishift logs
Flag --address has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --allow-privileged has been deprecated, will be removed in a future version
Flag --anonymous-auth has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authentication-token-webhook has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authentication-token-webhook-cache-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-webhook-cache-authorized-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-webhook-cache-unauthorized-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cadvisor-port has been deprecated, The default will change to 0 (disabled) in 1.11, and the cadvisor port will be removed entirely in 1.12
Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --client-ca-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cluster-domain has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --healthz-bind-address has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --healthz-port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --host-ipc-sources has been deprecated, will be removed in a future version
Flag --host-ipc-sources has been deprecated, will be removed in a future version
Flag --host-network-sources has been deprecated, will be removed in a future version
Flag --host-network-sources has been deprecated, will be removed in a future version
Flag --host-pid-sources has been deprecated, will be removed in a future version
Flag --host-pid-sources has been deprecated, will be removed in a future version
Flag --http-check-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --iptables-masquerade-bit has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --max-pods has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --read-only-port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cert-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cipher-suites has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cipher-suites has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cipher-suites has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cipher-suites has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cipher-suites has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cipher-suites has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cipher-suites has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cipher-suites has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cipher-suites has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cipher-suites has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cipher-suites has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cipher-suites has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cipher-suites has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cipher-suites has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cipher-suites has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cipher-suites has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-min-version has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-private-key-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-manifest-path has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --file-check-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cluster-dns has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
I0108 20:22:54.491513    5161 server.go:417] Version: v1.11.0+d4cacc0
I0108 20:22:54.491737    5161 plugins.go:97] No cloud provider specified.
I0108 20:22:54.531350    5161 server.go:657] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
I0108 20:22:54.531874    5161 container_manager_linux.go:243] container manager verified user specified cgroup-root exists: []
I0108 20:22:54.531894    5161 container_manager_linux.go:248] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: ContainerRuntime:docker CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd KubeletRootDir:/var/lib/minishift/base/openshift.local.volumes ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>} {Signal:imagefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.15} GracePeriod:0s MinReclaim:<nil>}]} QOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerReconcilePeriod:10s ExperimentalPodPidsLimit:-1 EnforceCPULimits:true}
I0108 20:22:54.532003    5161 container_manager_linux.go:267] Creating device plugin manager: true
I0108 20:22:54.532038    5161 state_mem.go:36] [cpumanager] initializing new in-memory state store
I0108 20:22:54.532354    5161 state_file.go:82] [cpumanager] state file: created new state file "/var/lib/minishift/base/openshift.local.volumes/cpu_manager_state"
I0108 20:22:54.532407    5161 kubelet.go:274] Adding pod path: /var/lib/origin/pod-manifests
I0108 20:22:54.532448    5161 kubelet.go:299] Watching apiserver
E0108 20:22:54.534985    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:54.535357    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:54.536253    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:22:54.545984    5161 client.go:75] Connecting to docker on unix:///var/run/docker.sock
I0108 20:22:54.546012    5161 client.go:104] Start docker client with request timeout=2m0s
W0108 20:22:54.548265    5161 docker_service.go:545] Hairpin mode set to "promiscuous-bridge" but kubenet is not enabled, falling back to "hairpin-veth"
I0108 20:22:54.548291    5161 docker_service.go:238] Hairpin mode set to "hairpin-veth"
W0108 20:22:54.548437    5161 cni.go:172] Unable to update cni config: No networks found in /etc/cni/net.d
I0108 20:22:54.552897    5161 docker_service.go:253] Docker cri networking managed by kubernetes.io/no-op
I0108 20:22:54.563113    5161 docker_service.go:258] Docker Info: &{ID:WF3F:RGWJ:YA4X:E3MA:C3X3:ZDVG:AXGU:53CU:AW5H:4KGS:J5WN:KYW5 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem xfs] [Supports d_type true] [Native Overlay Diff true]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host macvlan null overlay] Authorization:[] Log:[]} MemoryLimit:true SwapLimit:true KernelMemory:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:30 OomKillDisable:true NGoroutines:29 SystemTime:2020-01-08T20:22:54.55585867Z LoggingDriver:journald CgroupDriver:systemd NEventsListener:0 KernelVersion:3.10.0-957.5.1.el7.x86_64 OperatingSystem:CentOS Linux 7 (Core) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc420a536c0 NCPU:2 MemTotal:3153039360 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy:http://10.120.136.40:8080 HTTPSProxy:http://10.120.136.40:8080 NoProxy:localhost,127.0.0.1,172.30.1.1,172.30.1.1,192.168.99.1/16,192.168.42.0/16,10.120.0.0/16,127.0.0.1,localhost,::1,192.168.42.41,vault-vault-demo.192.168.42.41.nip.io,192.168.42.205,vault-vault-demo.192.168.42.205.nip.io,192.168.42.233,vault-vault-demo.192.168.42.233.nip.io Name:minishift Labels:[provider=kvm] ExperimentalBuild:false ServerVersion:1.13.1 ClusterStore: ClusterAdvertise: Runtimes:map[docker-runc:{Path:/usr/libexec/docker/docker-runc-current Args:[]} runc:{Path:docker-runc Args:[]}] DefaultRuntime:docker-runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:0xc420b39400} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:aa8187dbd3b7ad67d8e5e3a15115d3eef43a7ed1} RuncCommit:{ID:290a33602b16ff2d1cc5339bc0297f0e094462ce Expected:9df8b306d01f59d3a8029be411de015b7304dd8f} InitCommit:{ID:N/A Expected:949e6facb77383876aeff8a6944dde66b3089574} SecurityOptions:[name=seccomp,profile=default name=selinux]}
I0108 20:22:54.563199    5161 docker_service.go:271] Setting cgroupDriver to systemd
I0108 20:22:54.578046    5161 kuberuntime_manager.go:186] Container runtime docker initialized, version: 1.13.1, apiVersion: 1.26.0
W0108 20:22:54.578643    5161 probe.go:270] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
I0108 20:22:54.579028    5161 csi_plugin.go:111] kubernetes.io/csi: plugin initializing...
I0108 20:22:54.580283    5161 server.go:995] Started kubelet
E0108 20:22:54.581043    5161 kubelet.go:1244] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data for container /
I0108 20:22:54.581669    5161 fs_resource_analyzer.go:66] Starting FS ResourceAnalyzer
I0108 20:22:54.581691    5161 status_manager.go:152] Starting to sync pod status with apiserver
I0108 20:22:54.581703    5161 kubelet.go:1741] Starting kubelet main sync loop.
I0108 20:22:54.581715    5161 kubelet.go:1758] skipping pod synchronization - [container runtime is down PLEG is not healthy: pleg was last seen active 2562047h47m16.854775807s ago; threshold is 3m0s]
I0108 20:22:54.581907    5161 server.go:129] Starting to listen on 0.0.0.0:10250
I0108 20:22:54.582381    5161 server.go:307] Adding debug handlers to kubelet server.
E0108 20:22:54.583116    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
I0108 20:22:54.583631    5161 volume_manager.go:247] Starting Kubelet Volume Manager
I0108 20:22:54.583912    5161 desired_state_of_world_populator.go:130] Desired state populator starts to run
I0108 20:22:54.683858    5161 kubelet.go:1758] skipping pod synchronization - [container runtime is down]
I0108 20:22:54.684803    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:54.689478    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:22:54.690030    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:22:54.807417    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:54.810123    5161 cpu_manager.go:155] [cpumanager] starting with none policy
I0108 20:22:54.810141    5161 cpu_manager.go:156] [cpumanager] reconciling every 10s
I0108 20:22:54.810152    5161 policy_none.go:42] [cpumanager] none policy: Start
I0108 20:22:54.884040    5161 kubelet.go:1758] skipping pod synchronization - [container runtime is down]
I0108 20:22:54.890853    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:54.893466    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:22:54.893925    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
Starting Device Plugin manager
W0108 20:22:54.913051    5161 manager.go:496] Failed to retrieve checkpoint for "kubelet_internal_checkpoint": checkpoint is not found
E0108 20:22:54.913495    5161 eviction_manager.go:243] eviction manager: failed to get get summary stats: failed to get node info: node "localhost" not found
E0108 20:22:55.167588    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
I0108 20:22:55.294066    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:55.296596    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:22:55.297042    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:55.535760    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:55.537180    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:55.542416    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:22:55.543359    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:55.549618    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:55.550412    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
W0108 20:22:55.555771    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:22:55.561193    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:55.561375    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
W0108 20:22:55.568986    5161 status_manager.go:482] Failed to get status for pod "master-etcd-localhost_kube-system(34b17db69b2b3877c9904b5340f1ae71)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-etcd-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:22:55.570051    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:55.570823    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
W0108 20:22:55.577032    5161 status_manager.go:482] Failed to get status for pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:22:55.578933    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
W0108 20:22:55.587030    5161 status_manager.go:482] Failed to get status for pod "kube-scheduler-localhost_kube-system(f903f642800a02b87385310221ffe91f)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:22:55.603752    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-config" (UniqueName: "kubernetes.io/host-path/34b17db69b2b3877c9904b5340f1ae71-master-config") pod "master-etcd-localhost" (UID: "34b17db69b2b3877c9904b5340f1ae71") 
I0108 20:22:55.603831    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-data" (UniqueName: "kubernetes.io/host-path/34b17db69b2b3877c9904b5340f1ae71-master-data") pod "master-etcd-localhost" (UID: "34b17db69b2b3877c9904b5340f1ae71") 
I0108 20:22:55.603865    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-cloud-provider" (UniqueName: "kubernetes.io/host-path/dfcadfa6552711112062fbf1121a691c-master-cloud-provider") pod "kube-controller-manager-localhost" (UID: "dfcadfa6552711112062fbf1121a691c") 
I0108 20:22:55.603895    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-config" (UniqueName: "kubernetes.io/host-path/f903f642800a02b87385310221ffe91f-master-config") pod "kube-scheduler-localhost" (UID: "f903f642800a02b87385310221ffe91f") 
I0108 20:22:55.603921    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-config" (UniqueName: "kubernetes.io/host-path/29e68324ed097a2c36aa5709e9b67154-master-config") pod "master-api-localhost" (UID: "29e68324ed097a2c36aa5709e9b67154") 
I0108 20:22:55.604143    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-cloud-provider" (UniqueName: "kubernetes.io/host-path/29e68324ed097a2c36aa5709e9b67154-master-cloud-provider") pod "master-api-localhost" (UID: "29e68324ed097a2c36aa5709e9b67154") 
I0108 20:22:55.604182    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-data" (UniqueName: "kubernetes.io/host-path/29e68324ed097a2c36aa5709e9b67154-master-data") pod "master-api-localhost" (UID: "29e68324ed097a2c36aa5709e9b67154") 
I0108 20:22:55.604212    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-config" (UniqueName: "kubernetes.io/host-path/dfcadfa6552711112062fbf1121a691c-master-config") pod "kube-controller-manager-localhost" (UID: "dfcadfa6552711112062fbf1121a691c") 
I0108 20:22:55.604244    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-cloud-provider" (UniqueName: "kubernetes.io/host-path/f903f642800a02b87385310221ffe91f-master-cloud-provider") pod "kube-scheduler-localhost" (UID: "f903f642800a02b87385310221ffe91f") 
I0108 20:22:56.097222    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:56.100009    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:22:56.100452    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:56.536776    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:56.544956    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:56.545023    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:57.537389    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:57.545613    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:57.546785    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:22:57.700685    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:22:57.703534    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:22:57.704080    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:58.538022    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:58.546286    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:58.547342    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:59.538692    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:59.547253    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:22:59.548017    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:00.544958    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:00.547850    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:00.548840    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:00.904241    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:00.907116    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:23:00.907516    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:01.545613    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:01.548496    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:01.549431    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:02.546322    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:02.552139    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:02.552197    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:03.547402    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:03.554294    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:03.554362    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:04.549019    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:04.554798    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:04.555966    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:04.913735    5161 eviction_manager.go:243] eviction manager: failed to get get summary stats: failed to get node info: node "localhost" not found
E0108 20:23:05.168314    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
E0108 20:23:05.549658    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:05.555329    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:05.556460    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:06.550419    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:06.556190    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:06.557037    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:07.307704    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:07.310569    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:23:07.311028    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:07.551056    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:07.556834    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:07.558259    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:08.551787    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:08.557508    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:08.558867    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:09.552546    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:09.558203    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:09.559467    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:10.553247    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:10.558915    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:10.560070    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:11.553946    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:11.559568    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:11.560572    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:12.554903    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:12.561627    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:12.561692    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:13.555724    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:13.562347    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:13.563330    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:14.311343    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:14.313740    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:23:14.314164    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:14.558219    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:14.563119    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:14.563985    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:14.913954    5161 eviction_manager.go:243] eviction manager: failed to get get summary stats: failed to get node info: node "localhost" not found
E0108 20:23:15.169259    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
E0108 20:23:15.559981    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:15.565210    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:15.569927    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:16.560707    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:16.569387    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:16.570638    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:17.563014    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:17.570462    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:17.571956    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:18.564279    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:18.573438    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:18.574028    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:19.564884    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:19.575367    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:19.575442    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:20.565636    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:20.576070    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:20.577333    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:21.314380    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:21.316848    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:23:21.317275    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:21.566350    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:21.576753    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:21.578052    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:22.567334    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:22.577514    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:22.578680    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:23.574402    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:23.579341    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:23.584707    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:24.575266    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:24.579984    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:24.585306    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:24.914178    5161 eviction_manager.go:243] eviction manager: failed to get get summary stats: failed to get node info: node "localhost" not found
I0108 20:23:25.027953    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
W0108 20:23:25.031524    5161 status_manager.go:482] Failed to get status for pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:25.170222    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
E0108 20:23:25.583557    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:25.584397    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:25.585837    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:26.584286    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:26.585571    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:26.587112    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:27.584950    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:27.586073    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:27.587721    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:28.317479    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:28.320010    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:23:28.320448    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:28.585504    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:28.586589    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:28.588291    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:29.586223    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:29.587450    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:29.589025    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:30.586961    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:30.588252    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:30.589534    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:31.066546    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
W0108 20:23:31.073665    5161 status_manager.go:482] Failed to get status for pod "master-etcd-localhost_kube-system(34b17db69b2b3877c9904b5340f1ae71)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-etcd-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:31.078603    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
W0108 20:23:31.083012    5161 status_manager.go:482] Failed to get status for pod "kube-scheduler-localhost_kube-system(f903f642800a02b87385310221ffe91f)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:31.587667    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:31.589040    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:31.590459    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:32.082601    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:32.083484    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
E0108 20:23:32.588489    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:32.589701    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:32.591092    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:33.589184    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:33.590267    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:33.592095    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:34.590557    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:34.590911    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:34.593763    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:34.914381    5161 eviction_manager.go:243] eviction manager: failed to get get summary stats: failed to get node info: node "localhost" not found
E0108 20:23:35.172594    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
I0108 20:23:35.320787    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:35.324415    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:23:35.324842    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:35.591328    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:35.592565    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:35.594502    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:36.592564    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:36.593720    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:36.595168    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:37.593352    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:37.594510    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:37.595590    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:38.594610    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:38.595063    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:38.596778    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:39.595306    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:39.596752    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:39.597909    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:40.596076    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:40.597284    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:40.598432    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:41.596851    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:41.598127    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:41.599333    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:42.325030    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:42.331421    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:23:42.331750    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:42.597615    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:42.598866    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:42.600048    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:43.600034    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:43.600452    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:43.600464    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:44.600769    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:44.602942    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:44.604047    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:44.914659    5161 eviction_manager.go:243] eviction manager: failed to get get summary stats: failed to get node info: node "localhost" not found
E0108 20:23:45.173443    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/default/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
E0108 20:23:45.603759    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:45.603852    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:45.605278    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:46.604635    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:46.605511    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:46.607372    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:47.607318    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:47.607323    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:47.608180    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:48.608457    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:48.608900    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:48.611364    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:49.331957    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:49.334086    5161 kubelet_node_status.go:79] Attempting to register node localhost
E0108 20:23:49.334485    5161 kubelet_node_status.go:103] Unable to register node "localhost" with API server: Post https://localhost:8443/api/v1/nodes: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:49.609141    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:49.611500    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:49.613790    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:50.609932    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:50.612169    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:50.614445    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:51.610555    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:51.612749    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:51.615034    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:52.611302    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:52.613350    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:52.615597    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:53.614757    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:53.614896    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:23:53.616495    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:23:54.215076    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
E0108 20:23:54.914911    5161 eviction_manager.go:243] eviction manager: failed to get get summary stats: failed to get node info: node "localhost" not found
I0108 20:23:55.221722    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:56.334641    5161 kubelet_node_status.go:269] Setting node annotation to enable volume controller attach/detach
I0108 20:23:56.338559    5161 kubelet_node_status.go:79] Attempting to register node localhost
I0108 20:24:03.565802    5161 kubelet_node_status.go:82] Successfully registered node localhost
I0108 20:24:03.587039    5161 reconciler.go:154] Reconciler: start to sync state
E0108 20:24:06.840034    5161 event.go:203] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"localhost.15e803018c7d9572", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"localhost", UID:"localhost", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"Starting", Message:"Starting kubelet.", Source:v1.EventSource{Component:"kubelet", Host:"localhost"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xbf7dac67a295e972, ext:290033945, loc:(*time.Location)(0x90a9e60)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xbf7dac67a295e972, ext:290033945, loc:(*time.Location)(0x90a9e60)}}, Count:1, Type:"Normal", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'namespaces "default" not found' (will not retry!)
E0108 20:24:07.307059    5161 event.go:203] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"localhost.15e8030192ff98db", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"localhost", UID:"localhost", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientDisk", Message:"Node localhost status is now: NodeHasSufficientDisk", Source:v1.EventSource{Component:"kubelet", Host:"localhost"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xbf7dac67a917ecdb, ext:399217774, loc:(*time.Location)(0x90a9e60)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xbf7dac67a917ecdb, ext:399217774, loc:(*time.Location)(0x90a9e60)}}, Count:1, Type:"Normal", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'namespaces "default" not found' (will not retry!)
E0108 20:24:07.499285    5161 event.go:203] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"localhost.15e8030192ffc9e3", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"localhost", UID:"localhost", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node localhost status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"localhost"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xbf7dac67a9181de3, ext:399230324, loc:(*time.Location)(0x90a9e60)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xbf7dac67a9181de3, ext:399230324, loc:(*time.Location)(0x90a9e60)}}, Count:1, Type:"Normal", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'namespaces "default" not found' (will not retry!)
E0108 20:24:07.863065    5161 event.go:203] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"localhost.15e8030192ffe318", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"localhost", UID:"localhost", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node localhost status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"localhost"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xbf7dac67a9183718, ext:399236782, loc:(*time.Location)(0x90a9e60)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xbf7dac67a9183718, ext:399236782, loc:(*time.Location)(0x90a9e60)}}, Count:1, Type:"Normal", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'namespaces "default" not found' (will not retry!)
I0108 20:24:36.160175    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-proxy-token-ncmnd" (UniqueName: "kubernetes.io/secret/e344d540-3254-11ea-b26d-525400ca0151-kube-proxy-token-ncmnd") pod "kube-proxy-qlz8n" (UID: "e344d540-3254-11ea-b26d-525400ca0151") 
I0108 20:24:36.160215    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "node-config" (UniqueName: "kubernetes.io/host-path/e340201c-3254-11ea-b26d-525400ca0151-node-config") pod "kube-dns-mmmt6" (UID: "e340201c-3254-11ea-b26d-525400ca0151") 
I0108 20:24:36.160234    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-dns-token-mrp85" (UniqueName: "kubernetes.io/secret/e340201c-3254-11ea-b26d-525400ca0151-kube-dns-token-mrp85") pod "kube-dns-mmmt6" (UID: "e340201c-3254-11ea-b26d-525400ca0151") 
I0108 20:24:36.160248    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "node-config" (UniqueName: "kubernetes.io/host-path/e344d540-3254-11ea-b26d-525400ca0151-node-config") pod "kube-proxy-qlz8n" (UID: "e344d540-3254-11ea-b26d-525400ca0151") 
I0108 20:24:36.370279    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "openshift-apiserver-token-dg6ph" (UniqueName: "kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-openshift-apiserver-token-dg6ph") pod "openshift-apiserver-6q9ps" (UID: "e3681100-3254-11ea-b26d-525400ca0151") 
I0108 20:24:36.370322    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-config" (UniqueName: "kubernetes.io/host-path/e3681100-3254-11ea-b26d-525400ca0151-master-config") pod "openshift-apiserver-6q9ps" (UID: "e3681100-3254-11ea-b26d-525400ca0151") 
I0108 20:24:36.370350    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-cloud-provider" (UniqueName: "kubernetes.io/host-path/e3681100-3254-11ea-b26d-525400ca0151-master-cloud-provider") pod "openshift-apiserver-6q9ps" (UID: "e3681100-3254-11ea-b26d-525400ca0151") 
I0108 20:24:36.370375    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "serving-cert" (UniqueName: "kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert") pod "openshift-apiserver-6q9ps" (UID: "e3681100-3254-11ea-b26d-525400ca0151") 
E0108 20:24:36.618632    5161 secret.go:198] Couldn't get secret openshift-apiserver/serving-cert: secrets "serving-cert" not found
E0108 20:24:36.618749    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\" (\"e3681100-3254-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:24:37.118702486 +0000 UTC m=+102.828486447 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\") pod \"openshift-apiserver-6q9ps\" (UID: \"e3681100-3254-11ea-b26d-525400ca0151\") : secrets \"serving-cert\" not found"
I0108 20:24:37.013734    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "serving-cert" (UniqueName: "kubernetes.io/secret/e3ab1aff-3254-11ea-b26d-525400ca0151-serving-cert") pod "openshift-service-cert-signer-operator-6d477f986b-wfzn8" (UID: "e3ab1aff-3254-11ea-b26d-525400ca0151") 
I0108 20:24:37.013843    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "openshift-service-cert-signer-operator-token-58289" (UniqueName: "kubernetes.io/secret/e3ab1aff-3254-11ea-b26d-525400ca0151-openshift-service-cert-signer-operator-token-58289") pod "openshift-service-cert-signer-operator-6d477f986b-wfzn8" (UID: "e3ab1aff-3254-11ea-b26d-525400ca0151") 
I0108 20:24:37.013872    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "config" (UniqueName: "kubernetes.io/configmap/e3ab1aff-3254-11ea-b26d-525400ca0151-config") pod "openshift-service-cert-signer-operator-6d477f986b-wfzn8" (UID: "e3ab1aff-3254-11ea-b26d-525400ca0151") 
E0108 20:24:37.261200    5161 secret.go:198] Couldn't get secret openshift-apiserver/serving-cert: secrets "serving-cert" not found
E0108 20:24:37.261388    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\" (\"e3681100-3254-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:24:38.261269339 +0000 UTC m=+103.971053299 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\") pod \"openshift-apiserver-6q9ps\" (UID: \"e3681100-3254-11ea-b26d-525400ca0151\") : secrets \"serving-cert\" not found"
E0108 20:24:38.344501    5161 secret.go:198] Couldn't get secret openshift-apiserver/serving-cert: secrets "serving-cert" not found
E0108 20:24:38.344597    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\" (\"e3681100-3254-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:24:40.344554192 +0000 UTC m=+106.054338146 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\") pod \"openshift-apiserver-6q9ps\" (UID: \"e3681100-3254-11ea-b26d-525400ca0151\") : secrets \"serving-cert\" not found"
E0108 20:24:40.383040    5161 secret.go:198] Couldn't get secret openshift-apiserver/serving-cert: secrets "serving-cert" not found
E0108 20:24:40.383128    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\" (\"e3681100-3254-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:24:44.383096357 +0000 UTC m=+110.092880324 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\") pod \"openshift-apiserver-6q9ps\" (UID: \"e3681100-3254-11ea-b26d-525400ca0151\") : secrets \"serving-cert\" not found"
E0108 20:24:44.417073    5161 secret.go:198] Couldn't get secret openshift-apiserver/serving-cert: secrets "serving-cert" not found
E0108 20:24:44.417165    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\" (\"e3681100-3254-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:24:52.417130854 +0000 UTC m=+118.126914829 (durationBeforeRetry 8s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\") pod \"openshift-apiserver-6q9ps\" (UID: \"e3681100-3254-11ea-b26d-525400ca0151\") : secrets \"serving-cert\" not found"
E0108 20:24:52.463562    5161 secret.go:198] Couldn't get secret openshift-apiserver/serving-cert: secrets "serving-cert" not found
E0108 20:24:52.463654    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\" (\"e3681100-3254-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:25:08.463616045 +0000 UTC m=+134.173400006 (durationBeforeRetry 16s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\") pod \"openshift-apiserver-6q9ps\" (UID: \"e3681100-3254-11ea-b26d-525400ca0151\") : secrets \"serving-cert\" not found"
E0108 20:25:10.132139    5161 secret.go:198] Couldn't get secret openshift-apiserver/serving-cert: secrets "serving-cert" not found
E0108 20:25:10.132232    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\" (\"e3681100-3254-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:25:42.132198794 +0000 UTC m=+167.841982752 (durationBeforeRetry 32s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/e3681100-3254-11ea-b26d-525400ca0151-serving-cert\") pod \"openshift-apiserver-6q9ps\" (UID: \"e3681100-3254-11ea-b26d-525400ca0151\") : secrets \"serving-cert\" not found"
I0108 20:25:18.092643    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "signing-key" (UniqueName: "kubernetes.io/secret/fc3b8724-3254-11ea-b26d-525400ca0151-signing-key") pod "service-serving-cert-signer-668c45d5f-l6mc8" (UID: "fc3b8724-3254-11ea-b26d-525400ca0151") 
I0108 20:25:18.092693    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "serving-cert" (UniqueName: "kubernetes.io/secret/fc3b8724-3254-11ea-b26d-525400ca0151-serving-cert") pod "service-serving-cert-signer-668c45d5f-l6mc8" (UID: "fc3b8724-3254-11ea-b26d-525400ca0151") 
I0108 20:25:18.092723    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "config" (UniqueName: "kubernetes.io/configmap/fc3b8724-3254-11ea-b26d-525400ca0151-config") pod "service-serving-cert-signer-668c45d5f-l6mc8" (UID: "fc3b8724-3254-11ea-b26d-525400ca0151") 
I0108 20:25:18.092753    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "service-serving-cert-signer-sa-token-q2lh5" (UniqueName: "kubernetes.io/secret/fc3b8724-3254-11ea-b26d-525400ca0151-service-serving-cert-signer-sa-token-q2lh5") pod "service-serving-cert-signer-668c45d5f-l6mc8" (UID: "fc3b8724-3254-11ea-b26d-525400ca0151") 
I0108 20:25:24.024408    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "config" (UniqueName: "kubernetes.io/configmap/ff5ce502-3254-11ea-b26d-525400ca0151-config") pod "apiservice-cabundle-injector-8ffbbb6dc-hpf6m" (UID: "ff5ce502-3254-11ea-b26d-525400ca0151") 
I0108 20:25:24.024474    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "signing-cabundle" (UniqueName: "kubernetes.io/configmap/ff5ce502-3254-11ea-b26d-525400ca0151-signing-cabundle") pod "apiservice-cabundle-injector-8ffbbb6dc-hpf6m" (UID: "ff5ce502-3254-11ea-b26d-525400ca0151") 
I0108 20:25:24.024492    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "serving-cert" (UniqueName: "kubernetes.io/secret/ff5ce502-3254-11ea-b26d-525400ca0151-serving-cert") pod "apiservice-cabundle-injector-8ffbbb6dc-hpf6m" (UID: "ff5ce502-3254-11ea-b26d-525400ca0151") 
I0108 20:25:24.024510    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "apiservice-cabundle-injector-sa-token-tvtdw" (UniqueName: "kubernetes.io/secret/ff5ce502-3254-11ea-b26d-525400ca0151-apiservice-cabundle-injector-sa-token-tvtdw") pod "apiservice-cabundle-injector-8ffbbb6dc-hpf6m" (UID: "ff5ce502-3254-11ea-b26d-525400ca0151") 
I0108 20:26:50.281215    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-config" (UniqueName: "kubernetes.io/host-path/3334e363-3255-11ea-b26d-525400ca0151-master-config") pod "openshift-controller-manager-cp9j7" (UID: "3334e363-3255-11ea-b26d-525400ca0151") 
I0108 20:26:50.281266    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "openshift-controller-manager-token-t74x8" (UniqueName: "kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8") pod "openshift-controller-manager-cp9j7" (UID: "3334e363-3255-11ea-b26d-525400ca0151") 
I0108 20:26:50.281291    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "master-cloud-provider" (UniqueName: "kubernetes.io/host-path/3334e363-3255-11ea-b26d-525400ca0151-master-cloud-provider") pod "openshift-controller-manager-cp9j7" (UID: "3334e363-3255-11ea-b26d-525400ca0151") 
I0108 20:27:16.905229    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "pvdir" (UniqueName: "kubernetes.io/host-path/3567cf6f-3255-11ea-b26d-525400ca0151-pvdir") pod "persistent-volume-setup-5mzds" (UID: "3567cf6f-3255-11ea-b26d-525400ca0151") 
I0108 20:27:16.905305    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "pvinstaller-token-zgfb9" (UniqueName: "kubernetes.io/secret/3567cf6f-3255-11ea-b26d-525400ca0151-pvinstaller-token-zgfb9") pod "persistent-volume-setup-5mzds" (UID: "3567cf6f-3255-11ea-b26d-525400ca0151") 
I0108 20:27:17.621594    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "openshift-web-console-operator-token-l72l6" (UniqueName: "kubernetes.io/secret/437c30e1-3255-11ea-b26d-525400ca0151-openshift-web-console-operator-token-l72l6") pod "openshift-web-console-operator-57986c9c4f-4tpgb" (UID: "437c30e1-3255-11ea-b26d-525400ca0151") 
I0108 20:27:17.621642    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "serving-cert" (UniqueName: "kubernetes.io/secret/437c30e1-3255-11ea-b26d-525400ca0151-serving-cert") pod "openshift-web-console-operator-57986c9c4f-4tpgb" (UID: "437c30e1-3255-11ea-b26d-525400ca0151") 
I0108 20:27:17.621670    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "config" (UniqueName: "kubernetes.io/configmap/437c30e1-3255-11ea-b26d-525400ca0151-config") pod "openshift-web-console-operator-57986c9c4f-4tpgb" (UID: "437c30e1-3255-11ea-b26d-525400ca0151") 
I0108 20:27:24.525031    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "deployer-token-kdk2h" (UniqueName: "kubernetes.io/secret/4735bc66-3255-11ea-b26d-525400ca0151-deployer-token-kdk2h") pod "docker-registry-1-deploy" (UID: "4735bc66-3255-11ea-b26d-525400ca0151") 
I0108 20:27:24.525092    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "deployer-token-kdk2h" (UniqueName: "kubernetes.io/secret/4734816d-3255-11ea-b26d-525400ca0151-deployer-token-kdk2h") pod "router-1-deploy" (UID: "4734816d-3255-11ea-b26d-525400ca0151") 
I0108 20:27:30.031259    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "serving-cert" (UniqueName: "kubernetes.io/secret/4a9f6dcd-3255-11ea-b26d-525400ca0151-serving-cert") pod "webconsole-777b884d95-fvssr" (UID: "4a9f6dcd-3255-11ea-b26d-525400ca0151") 
I0108 20:27:30.031368    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "webconsole-config" (UniqueName: "kubernetes.io/configmap/4a9f6dcd-3255-11ea-b26d-525400ca0151-webconsole-config") pod "webconsole-777b884d95-fvssr" (UID: "4a9f6dcd-3255-11ea-b26d-525400ca0151") 
I0108 20:27:30.031407    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "webconsole-token-mhn7m" (UniqueName: "kubernetes.io/secret/4a9f6dcd-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m") pod "webconsole-777b884d95-fvssr" (UID: "4a9f6dcd-3255-11ea-b26d-525400ca0151") 
I0108 20:27:33.300050    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "deployer-token-kdk2h" (UniqueName: "kubernetes.io/secret/4734816d-3255-11ea-b26d-525400ca0151-deployer-token-kdk2h") pod "4734816d-3255-11ea-b26d-525400ca0151" (UID: "4734816d-3255-11ea-b26d-525400ca0151") 
I0108 20:27:33.924005    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/4734816d-3255-11ea-b26d-525400ca0151-deployer-token-kdk2h" (OuterVolumeSpecName: "deployer-token-kdk2h") pod "4734816d-3255-11ea-b26d-525400ca0151" (UID: "4734816d-3255-11ea-b26d-525400ca0151"). InnerVolumeSpecName "deployer-token-kdk2h". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0108 20:27:33.957008    5161 reconciler.go:301] Volume detached for volume "deployer-token-kdk2h" (UniqueName: "kubernetes.io/secret/4734816d-3255-11ea-b26d-525400ca0151-deployer-token-kdk2h") on node "localhost" DevicePath ""
W0108 20:27:34.826020    5161 pod_container_deletor.go:75] Container "0fcdde98ecd8ea48da4bc507ae32ee40981fffdd6500814c9067f327ec451e7b" not found in pod's containers
I0108 20:27:36.997032    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "registry-token-b8dwv" (UniqueName: "kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv") pod "docker-registry-1-tlcmc" (UID: "4ef4964d-3255-11ea-b26d-525400ca0151") 
I0108 20:27:36.997089    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "registry-storage" (UniqueName: "kubernetes.io/host-path/4ef4964d-3255-11ea-b26d-525400ca0151-registry-storage") pod "docker-registry-1-tlcmc" (UID: "4ef4964d-3255-11ea-b26d-525400ca0151") 
E0108 20:27:54.817588    5161 fsHandler.go:121] failed to collect filesystem stats - rootDiskErr: du command failed on /rootfs/var/lib/docker/overlay2/7ed74e29e518244187cf0c46bd321874f3c43b0218cdf7e11d656d5e330c7577/diff with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/overlay2/7ed74e29e518244187cf0c46bd321874f3c43b0218cdf7e11d656d5e330c7577/diff': No such file or directory
 - exit status 1, rootInodeErr: cmd [ionice -c3 nice -n 19 find /rootfs/var/lib/docker/overlay2/7ed74e29e518244187cf0c46bd321874f3c43b0218cdf7e11d656d5e330c7577/diff -xdev -printf .] failed. stderr: find: '/rootfs/var/lib/docker/overlay2/7ed74e29e518244187cf0c46bd321874f3c43b0218cdf7e11d656d5e330c7577/diff': No such file or directory
; err: exit status 1, extraDiskErr: du command failed on /rootfs/var/lib/docker/containers/f3f7fc0b72cc1f42b8f67e78b73be25bad97b099c776333fdeb97e21a8629f05 with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/containers/f3f7fc0b72cc1f42b8f67e78b73be25bad97b099c776333fdeb97e21a8629f05': No such file or directory
 - exit status 1
I0108 20:27:59.340979    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "webconsole-token-mhn7m" (UniqueName: "kubernetes.io/secret/4a9f6dcd-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m") pod "4a9f6dcd-3255-11ea-b26d-525400ca0151" (UID: "4a9f6dcd-3255-11ea-b26d-525400ca0151") 
I0108 20:27:59.342031    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "serving-cert" (UniqueName: "kubernetes.io/secret/4a9f6dcd-3255-11ea-b26d-525400ca0151-serving-cert") pod "4a9f6dcd-3255-11ea-b26d-525400ca0151" (UID: "4a9f6dcd-3255-11ea-b26d-525400ca0151") 
I0108 20:27:59.342068    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "webconsole-config" (UniqueName: "kubernetes.io/configmap/4a9f6dcd-3255-11ea-b26d-525400ca0151-webconsole-config") pod "4a9f6dcd-3255-11ea-b26d-525400ca0151" (UID: "4a9f6dcd-3255-11ea-b26d-525400ca0151") 
I0108 20:27:59.587842    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/4a9f6dcd-3255-11ea-b26d-525400ca0151-webconsole-config" (OuterVolumeSpecName: "webconsole-config") pod "4a9f6dcd-3255-11ea-b26d-525400ca0151" (UID: "4a9f6dcd-3255-11ea-b26d-525400ca0151"). InnerVolumeSpecName "webconsole-config". PluginName "kubernetes.io/configmap", VolumeGidValue ""
E0108 20:27:59.610158    5161 remote_runtime.go:278] ContainerStatus "e3374a630d8aa4282a7386a8d47eab203e1e32d526fb8328294ae90a6796e158" from runtime service failed: rpc error: code = Unknown desc = Error: No such container: e3374a630d8aa4282a7386a8d47eab203e1e32d526fb8328294ae90a6796e158
I0108 20:27:59.611021    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/4a9f6dcd-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m" (OuterVolumeSpecName: "webconsole-token-mhn7m") pod "4a9f6dcd-3255-11ea-b26d-525400ca0151" (UID: "4a9f6dcd-3255-11ea-b26d-525400ca0151"). InnerVolumeSpecName "webconsole-token-mhn7m". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0108 20:27:59.667353    5161 reconciler.go:301] Volume detached for volume "webconsole-config" (UniqueName: "kubernetes.io/configmap/4a9f6dcd-3255-11ea-b26d-525400ca0151-webconsole-config") on node "localhost" DevicePath ""
I0108 20:27:59.668063    5161 reconciler.go:301] Volume detached for volume "webconsole-token-mhn7m" (UniqueName: "kubernetes.io/secret/4a9f6dcd-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m") on node "localhost" DevicePath ""
I0108 20:27:59.693781    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/4a9f6dcd-3255-11ea-b26d-525400ca0151-serving-cert" (OuterVolumeSpecName: "serving-cert") pod "4a9f6dcd-3255-11ea-b26d-525400ca0151" (UID: "4a9f6dcd-3255-11ea-b26d-525400ca0151"). InnerVolumeSpecName "serving-cert". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0108 20:27:59.769321    5161 reconciler.go:301] Volume detached for volume "serving-cert" (UniqueName: "kubernetes.io/secret/4a9f6dcd-3255-11ea-b26d-525400ca0151-serving-cert") on node "localhost" DevicePath ""
I0108 20:28:05.679791    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "serving-cert" (UniqueName: "kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert") pod "webconsole-85c9b79fdf-56nvm" (UID: "601081ba-3255-11ea-b26d-525400ca0151") 
I0108 20:28:05.681760    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "webconsole-config" (UniqueName: "kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config") pod "webconsole-85c9b79fdf-56nvm" (UID: "601081ba-3255-11ea-b26d-525400ca0151") 
I0108 20:28:05.683546    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "webconsole-token-mhn7m" (UniqueName: "kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m") pod "webconsole-85c9b79fdf-56nvm" (UID: "601081ba-3255-11ea-b26d-525400ca0151") 
W0108 20:28:06.473485    5161 status_manager.go:482] Failed to get status for pod "webconsole-85c9b79fdf-56nvm_openshift-web-console(601081ba-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-web-console/pods/webconsole-85c9b79fdf-56nvm: unexpected EOF
E0108 20:28:06.473565    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-token-mhn7m: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: unexpected EOF
E0108 20:28:06.473661    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:06.97362722 +0000 UTC m=+312.683411172 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"webconsole-token-mhn7m\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: unexpected EOF"
E0108 20:28:06.474040    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-serving-cert: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: unexpected EOF
E0108 20:28:06.474116    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:06.974085011 +0000 UTC m=+312.683868970 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: unexpected EOF"
E0108 20:28:06.474156    5161 configmap.go:199] Couldn't get configMap openshift-web-console/webconsole-config: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: unexpected EOF
E0108 20:28:06.474210    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:06.974186364 +0000 UTC m=+312.683970324 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"webconsole-config\" (UniqueName: \"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: unexpected EOF"
E0108 20:28:06.474275    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/openshift-web-console/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
E0108 20:28:06.474331    5161 reflector.go:253] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to watch *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&resourceVersion=2209&timeoutSeconds=385&watch=true: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:06.474379    5161 reflector.go:253] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to watch *v1.Service: Get https://localhost:8443/api/v1/services?resourceVersion=1535&timeoutSeconds=458&watch=true: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:06.474416    5161 reflector.go:253] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to watch *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&resourceVersion=2189&timeoutSeconds=373&watch=true: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:07.042073    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-token-mhn7m: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:07.042176    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:08.042142673 +0000 UTC m=+313.751926631 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"webconsole-token-mhn7m\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:07.042243    5161 configmap.go:199] Couldn't get configMap openshift-web-console/webconsole-config: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:07.042300    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:08.042274766 +0000 UTC m=+313.752058726 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"webconsole-config\" (UniqueName: \"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:07.042346    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-serving-cert: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:07.042398    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:08.042372083 +0000 UTC m=+313.752156044 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:07.477981    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:07.483674    5161 status_manager.go:482] Failed to get status for pod "docker-registry-1-deploy_default(4735bc66-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/default/pods/docker-registry-1-deploy: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:07.504091    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:07.504173    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:07.582775    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:28:07.670996    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "deployer-token-kdk2h" (UniqueName: "kubernetes.io/secret/4735bc66-3255-11ea-b26d-525400ca0151-deployer-token-kdk2h") pod "4735bc66-3255-11ea-b26d-525400ca0151" (UID: "4735bc66-3255-11ea-b26d-525400ca0151") 
I0108 20:28:07.765283    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/4735bc66-3255-11ea-b26d-525400ca0151-deployer-token-kdk2h" (OuterVolumeSpecName: "deployer-token-kdk2h") pod "4735bc66-3255-11ea-b26d-525400ca0151" (UID: "4735bc66-3255-11ea-b26d-525400ca0151"). InnerVolumeSpecName "deployer-token-kdk2h". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0108 20:28:07.774595    5161 reconciler.go:301] Volume detached for volume "deployer-token-kdk2h" (UniqueName: "kubernetes.io/secret/4735bc66-3255-11ea-b26d-525400ca0151-deployer-token-kdk2h") on node "localhost" DevicePath ""
I0108 20:28:07.905940    5161 kuberuntime_manager.go:513] Container {Name:api Image:openshift/origin-hypershift:v3.11.0 Command:[/bin/bash -c] Args:[#!/bin/bash
set -euo pipefail
if [[ -f /etc/origin/master/master.env ]]; then
  set -o allexport
  source /etc/origin/master/master.env
fi
exec hypershift openshift-kube-apiserver --config=/etc/origin/master/master-config.yaml
] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:master-data ReadOnly:false MountPath:/var/lib/origin/ SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:healthz,Port:8443,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:28:07.906071    5161 kuberuntime_manager.go:757] checking backoff for container "api" in pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)"
E0108 20:28:08.116033    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-serving-cert: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:08.116128    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:10.116095947 +0000 UTC m=+315.825879900 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:08.116186    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-token-mhn7m: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:08.116244    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:10.116218748 +0000 UTC m=+315.826002701 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"webconsole-token-mhn7m\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:08.116307    5161 configmap.go:199] Couldn't get configMap openshift-web-console/webconsole-config: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:08.116382    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:10.116342601 +0000 UTC m=+315.826126559 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"webconsole-config\" (UniqueName: \"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:08.484131    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:08.509850    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:08.509915    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:08.697089    5161 pod_container_deletor.go:75] Container "3b6a9602b5126578d7f7c12c683a2953c505bc9541ce682bb61462b0972f9632" not found in pod's containers
E0108 20:28:09.485580    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:09.510383    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:09.512880    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:10.187406    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-serving-cert: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:10.187503    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:14.187466664 +0000 UTC m=+319.897250623 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:10.187567    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-token-mhn7m: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:10.187632    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:14.187605654 +0000 UTC m=+319.897389605 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"webconsole-token-mhn7m\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:10.187685    5161 configmap.go:199] Couldn't get configMap openshift-web-console/webconsole-config: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:10.187741    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:14.187714043 +0000 UTC m=+319.897498002 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"webconsole-config\" (UniqueName: \"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:10.489782    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:10.521129    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:10.521196    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:11.490455    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:11.525339    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:11.525926    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:12.493600    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:12.535134    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:12.535201    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:12.877028    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/openshift-web-console/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
E0108 20:28:13.495590    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.539746    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.539826    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.706395    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?resourceVersion=0&timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.706730    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.707021    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.707257    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.707477    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.707489    5161 kubelet_node_status.go:379] Unable to update node status: update node status exceeds retry count
W0108 20:28:13.858309    5161 status_manager.go:482] Failed to get status for pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:13.894303    5161 status_manager.go:482] Failed to get status for pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/pods/openshift-controller-manager-cp9j7: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.944704    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:13.944838    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:14.444777516 +0000 UTC m=+320.154561478 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
I0108 20:28:14.160026    5161 kuberuntime_manager.go:513] Container {Name:controllers Image:openshift/origin-hyperkube:v3.11.0 Command:[hyperkube kube-controller-manager] Args:[--enable-dynamic-provisioning=true --use-service-account-credentials=true --leader-elect-retry-period=3s --leader-elect-resource-lock=configmaps --controllers=* --controllers=-ttl --controllers=-bootstrapsigner --controllers=-tokencleaner --controllers=-horizontalpodautoscaling --pod-eviction-timeout=5m --cluster-signing-key-file= --cluster-signing-cert-file= --experimental-cluster-signing-duration=720h --root-ca-file=/etc/origin/master/ca-bundle.crt --port=10252 --service-account-private-key-file=/etc/origin/master/serviceaccounts.private.key --kubeconfig=/etc/origin/master/openshift-master.kubeconfig --openshift-config=/etc/origin/master/master-config.yaml] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:healthz,Port:10252,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:28:14.160141    5161 kuberuntime_manager.go:757] checking backoff for container "controllers" in pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)"
I0108 20:28:14.195270    5161 kuberuntime_manager.go:513] Container {Name:c Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift openshift-controller-manager] Args:[--config=/etc/origin/master/master-config.yaml --v=0] WorkingDir: Ports:[{Name: HostPort:8444 ContainerPort:8444 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:openshift-controller-manager-token-t74x8 ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:8444,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:28:14.195380    5161 kuberuntime_manager.go:757] checking backoff for container "c" in pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)"
E0108 20:28:14.265994    5161 configmap.go:199] Couldn't get configMap openshift-web-console/webconsole-config: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:14.266093    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:22.266060385 +0000 UTC m=+327.975844344 (durationBeforeRetry 8s). Error: "MountVolume.SetUp failed for volume \"webconsole-config\" (UniqueName: \"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:14.266143    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-token-mhn7m: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:14.266151    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-serving-cert: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:14.266209    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:22.266183567 +0000 UTC m=+327.975967518 (durationBeforeRetry 8s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:14.266241    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:22.266216651 +0000 UTC m=+327.976000607 (durationBeforeRetry 8s). Error: "MountVolume.SetUp failed for volume \"webconsole-token-mhn7m\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:14.524251    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:14.524338    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:14.524432    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:15.524392884 +0000 UTC m=+321.234176848 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:14.542636    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:14.576304    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:14.592577    5161 status_manager.go:482] Failed to get status for pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:14.593380    5161 status_manager.go:482] Failed to get status for pod "webconsole-85c9b79fdf-56nvm_openshift-web-console(601081ba-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-web-console/pods/webconsole-85c9b79fdf-56nvm: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:14.701956    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:14.896394    5161 status_manager.go:482] Failed to get status for pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/pods/openshift-controller-manager-cp9j7: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:15.895459    5161 status_manager.go:482] Failed to get status for pod "docker-registry-1-deploy_default(4735bc66-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/default/pods/docker-registry-1-deploy: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:16.096018    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:16.295514    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:16.496684    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:16.701307    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:16.701419    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:18.701381428 +0000 UTC m=+324.411165388 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
W0108 20:28:16.896296    5161 status_manager.go:482] Failed to get status for pod "openshift-web-console-operator-57986c9c4f-4tpgb_openshift-core-operators(437c30e1-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-core-operators/pods/openshift-web-console-operator-57986c9c4f-4tpgb: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:28:17.137981    5161 kuberuntime_manager.go:513] Container {Name:operator Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift experimental openshift-webconsole-operator] Args:[--config=/var/run/configmaps/config/operator-config.yaml -v=0] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:config ReadOnly:false MountPath:/var/run/configmaps/config SubPath: MountPropagation:<nil>} {Name:openshift-web-console-operator-token-l72l6 ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:28:17.138089    5161 kuberuntime_manager.go:757] checking backoff for container "operator" in pod "openshift-web-console-operator-57986c9c4f-4tpgb_openshift-core-operators(437c30e1-3255-11ea-b26d-525400ca0151)"
I0108 20:28:17.139700    5161 kuberuntime_manager.go:513] Container {Name:scheduler Image:openshift/origin-hyperkube:v3.11.0 Command:[hyperkube kube-scheduler] Args:[--leader-elect=true --leader-elect-resource-lock=configmaps --port=10251 --kubeconfig=/etc/origin/master/openshift-master.kubeconfig --policy-config-file=] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:healthz,Port:10251,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:28:17.139778    5161 kuberuntime_manager.go:757] checking backoff for container "scheduler" in pod "kube-scheduler-localhost_kube-system(f903f642800a02b87385310221ffe91f)"
I0108 20:28:17.139946    5161 kuberuntime_manager.go:513] Container {Name:operator Image:openshift/origin-service-serving-cert-signer:v3.11 Command:[service-serving-cert-signer operator] Args:[--config=/var/run/configmaps/config/operator-config.yaml -v=4] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:config ReadOnly:false MountPath:/var/run/configmaps/config SubPath: MountPropagation:<nil>} {Name:openshift-service-cert-signer-operator-token-58289 ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:28:17.140025    5161 kuberuntime_manager.go:757] checking backoff for container "operator" in pod "openshift-service-cert-signer-operator-6d477f986b-wfzn8_openshift-core-operators(e3ab1aff-3254-11ea-b26d-525400ca0151)"
W0108 20:28:17.500339    5161 status_manager.go:482] Failed to get status for pod "kube-scheduler-localhost_kube-system(f903f642800a02b87385310221ffe91f)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:17.707041    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:17.897316    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:18.096326    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:18.298870    5161 status_manager.go:482] Failed to get status for pod "openshift-service-cert-signer-operator-6d477f986b-wfzn8_openshift-core-operators(e3ab1aff-3254-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-core-operators/pods/openshift-service-cert-signer-operator-6d477f986b-wfzn8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:18.707741    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:18.805554    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:18.805660    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:22.805620175 +0000 UTC m=+328.515404125 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:18.898512    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:19.097079    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:19.708390    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:19.899140    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:20.097764    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:20.709149    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:20.899766    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:21.098496    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:21.709930    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:21.909793    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:22.103425    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:22.354230    5161 configmap.go:199] Couldn't get configMap openshift-web-console/webconsole-config: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:22.354343    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:38.354307728 +0000 UTC m=+344.064091686 (durationBeforeRetry 16s). Error: "MountVolume.SetUp failed for volume \"webconsole-config\" (UniqueName: \"kubernetes.io/configmap/601081ba-3255-11ea-b26d-525400ca0151-webconsole-config\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:22.354409    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-serving-cert: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:22.354487    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:38.35446054 +0000 UTC m=+344.064244491 (durationBeforeRetry 16s). Error: "MountVolume.SetUp failed for volume \"serving-cert\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-serving-cert\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:22.354537    5161 secret.go:198] Couldn't get secret openshift-web-console/webconsole-token-mhn7m: Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:22.354592    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\" (\"601081ba-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:38.354566307 +0000 UTC m=+344.064350267 (durationBeforeRetry 16s). Error: "MountVolume.SetUp failed for volume \"webconsole-token-mhn7m\" (UniqueName: \"kubernetes.io/secret/601081ba-3255-11ea-b26d-525400ca0151-webconsole-token-mhn7m\") pod \"webconsole-85c9b79fdf-56nvm\" (UID: \"601081ba-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:22.715394    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:22.877647    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/openshift-web-console/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
E0108 20:28:22.892667    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:22.892769    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:30.892731894 +0000 UTC m=+336.602515852 (durationBeforeRetry 8s). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:22.911386    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:23.106267    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:23.708447    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?resourceVersion=0&timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:23.708780    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:23.709091    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:23.709339    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:23.709569    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:23.709580    5161 kubelet_node_status.go:379] Unable to update node status: update node status exceeds retry count
E0108 20:28:23.716377    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:23.912086    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:24.107009    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:24.587933    5161 status_manager.go:482] Failed to get status for pod "openshift-web-console-operator-57986c9c4f-4tpgb_openshift-core-operators(437c30e1-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-core-operators/pods/openshift-web-console-operator-57986c9c4f-4tpgb: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:24.588294    5161 status_manager.go:482] Failed to get status for pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:24.588562    5161 status_manager.go:482] Failed to get status for pod "kube-scheduler-localhost_kube-system(f903f642800a02b87385310221ffe91f)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:24.588834    5161 status_manager.go:482] Failed to get status for pod "webconsole-85c9b79fdf-56nvm_openshift-web-console(601081ba-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-web-console/pods/webconsole-85c9b79fdf-56nvm: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:24.589084    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:24.589330    5161 status_manager.go:482] Failed to get status for pod "openshift-service-cert-signer-operator-6d477f986b-wfzn8_openshift-core-operators(e3ab1aff-3254-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-core-operators/pods/openshift-service-cert-signer-operator-6d477f986b-wfzn8: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:24.589589    5161 status_manager.go:482] Failed to get status for pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/pods/openshift-controller-manager-cp9j7: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:24.589851    5161 status_manager.go:482] Failed to get status for pod "docker-registry-1-deploy_default(4735bc66-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/default/pods/docker-registry-1-deploy: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:24.719438    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:24.914777    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:25.096387    5161 status_manager.go:482] Failed to get status for pod "docker-registry-1-tlcmc_default(4ef4964d-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/default/pods/docker-registry-1-tlcmc: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:25.299338    5161 secret.go:198] Couldn't get secret default/registry-token-b8dwv: Get https://localhost:8443/api/v1/namespaces/default/secrets/registry-token-b8dwv: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:25.299447    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv\" (\"4ef4964d-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:25.799410546 +0000 UTC m=+331.509194506 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"registry-token-b8dwv\" (UniqueName: \"kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv\") pod \"docker-registry-1-tlcmc\" (UID: \"4ef4964d-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/default/secrets/registry-token-b8dwv: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:25.504789    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:25.722423    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:25.895488    5161 secret.go:198] Couldn't get secret default/registry-token-b8dwv: Get https://localhost:8443/api/v1/namespaces/default/secrets/registry-token-b8dwv: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:25.895581    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv\" (\"4ef4964d-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:26.895556337 +0000 UTC m=+332.605340288 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"registry-token-b8dwv\" (UniqueName: \"kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv\") pod \"docker-registry-1-tlcmc\" (UID: \"4ef4964d-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/default/secrets/registry-token-b8dwv: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:28:26.095477    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:26.505597    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:26.723902    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:27.017057    5161 secret.go:198] Couldn't get secret default/registry-token-b8dwv: Get https://localhost:8443/api/v1/namespaces/default/secrets/registry-token-b8dwv: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:27.017162    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv\" (\"4ef4964d-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:29.017125828 +0000 UTC m=+334.726909790 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"registry-token-b8dwv\" (UniqueName: \"kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv\") pod \"docker-registry-1-tlcmc\" (UID: \"4ef4964d-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/default/secrets/registry-token-b8dwv: dial tcp 127.0.0.1:8443: connect: connection refused"
W0108 20:28:27.145390    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:33.173657    5161 fsHandler.go:121] failed to collect filesystem stats - rootDiskErr: du command failed on /rootfs/var/lib/docker/overlay2/08596b42921e7870e1883c2e6819353b00226f3a6cb0fb04e763eac9eb2c2160/diff with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/overlay2/08596b42921e7870e1883c2e6819353b00226f3a6cb0fb04e763eac9eb2c2160/diff': No such file or directory
 - exit status 1, rootInodeErr: cmd [ionice -c3 nice -n 19 find /rootfs/var/lib/docker/overlay2/08596b42921e7870e1883c2e6819353b00226f3a6cb0fb04e763eac9eb2c2160/diff -xdev -printf .] failed. stderr: find: '/rootfs/var/lib/docker/overlay2/08596b42921e7870e1883c2e6819353b00226f3a6cb0fb04e763eac9eb2c2160/diff': No such file or directory
; err: exit status 1, extraDiskErr: du command failed on /rootfs/var/lib/docker/containers/f540e0650efd421e1968c2b1dc8d2977f6af99b1b9dea73cf51ecf3e9d4d8650 with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/containers/f540e0650efd421e1968c2b1dc8d2977f6af99b1b9dea73cf51ecf3e9d4d8650': No such file or directory
 - exit status 1
E0108 20:28:37.177555    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: net/http: TLS handshake timeout
W0108 20:28:37.404043    5161 status_manager.go:482] Failed to get status for pod "docker-registry-1-tlcmc_default(4ef4964d-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/default/pods/docker-registry-1-tlcmc: net/http: TLS handshake timeout
E0108 20:28:37.516048    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: net/http: TLS handshake timeout
E0108 20:28:37.725073    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: net/http: TLS handshake timeout
E0108 20:28:38.323265    5161 secret.go:198] Couldn't get secret default/registry-token-b8dwv: secrets "registry-token-b8dwv" is forbidden: User "system:node:localhost" cannot get secrets in the namespace "default": no path found to object
no RBAC policy matched
E0108 20:28:38.323357    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv\" (\"4ef4964d-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:28:42.32332111 +0000 UTC m=+348.033105072 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"registry-token-b8dwv\" (UniqueName: \"kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv\") pod \"docker-registry-1-tlcmc\" (UID: \"4ef4964d-3255-11ea-b26d-525400ca0151\") : secrets \"registry-token-b8dwv\" is forbidden: User \"system:node:localhost\" cannot get secrets in the namespace \"default\": no path found to object\nno RBAC policy matched"
E0108 20:28:48.417037    5161 event.go:203] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"webconsole-85c9b79fdf-56nvm.15e8034a2ac873c1", GenerateName:"", Namespace:"openshift-web-console", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"openshift-web-console", Name:"webconsole-85c9b79fdf-56nvm", UID:"601081ba-3255-11ea-b26d-525400ca0151", APIVersion:"v1", ResourceVersion:"2209", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"webconsole-token-mhn7m\" : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: unexpected EOF", Source:v1.EventSource{Component:"kubelet", Host:"localhost"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb59c3a97c1, ext:312183385943, loc:(*time.Location)(0x90a9e60)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb59c3a97c1, ext:312183385943, loc:(*time.Location)(0x90a9e60)}}, Count:1, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "webconsole-85c9b79fdf-56nvm.15e8034a2ac873c1" is forbidden: caches not synchronized' (will not retry!)
E0108 20:28:49.944311    5161 fsHandler.go:121] failed to collect filesystem stats - rootDiskErr: du command failed on /rootfs/var/lib/docker/overlay2/09402e4780d187d94e37dd2158a81eb0c3776a1dcc00610bdd36c84684963053/diff with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/overlay2/09402e4780d187d94e37dd2158a81eb0c3776a1dcc00610bdd36c84684963053/diff': No such file or directory
 - exit status 1, rootInodeErr: cmd [ionice -c3 nice -n 19 find /rootfs/var/lib/docker/overlay2/09402e4780d187d94e37dd2158a81eb0c3776a1dcc00610bdd36c84684963053/diff -xdev -printf .] failed. stderr: find: '/rootfs/var/lib/docker/overlay2/09402e4780d187d94e37dd2158a81eb0c3776a1dcc00610bdd36c84684963053/diff': No such file or directory
; err: exit status 1, extraDiskErr: du command failed on /rootfs/var/lib/docker/containers/e3374a630d8aa4282a7386a8d47eab203e1e32d526fb8328294ae90a6796e158 with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/containers/e3374a630d8aa4282a7386a8d47eab203e1e32d526fb8328294ae90a6796e158': No such file or directory
 - exit status 1
I0108 20:28:56.857108    5161 kuberuntime_manager.go:513] Container {Name:apiserver Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift openshift-apiserver] Args:[--config=/etc/origin/master/master-config.yaml -v=0] WorkingDir: Ports:[{Name: HostPort:8445 ContainerPort:8445 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:ADDITIONAL_ALLOWED_REGISTRIES Value:registry.centos.org ValueFrom:nil}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:serving-cert ReadOnly:false MountPath:/var/serving-cert SubPath: MountPropagation:<nil>} {Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:openshift-apiserver-token-dg6ph ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:8445,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:28:56.857424    5161 kuberuntime_manager.go:757] checking backoff for container "apiserver" in pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"
E0108 20:28:57.890397    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/openshift-web-console/events: unexpected EOF; some request body already written' (may retry after sleeping)
E0108 20:28:57.890620    5161 reflector.go:253] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to watch *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&resourceVersion=2261&timeoutSeconds=509&watch=true: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:57.890677    5161 reflector.go:253] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to watch *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&resourceVersion=2246&timeoutSeconds=309&watch=true: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:57.890727    5161 reflector.go:253] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to watch *v1.Service: Get https://localhost:8443/api/v1/services?resourceVersion=2213&timeoutSeconds=588&watch=true: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:28:58.671567    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:58.895872    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:58.895941    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:58.896008    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?resourceVersion=0&timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:58.896292    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:58.896537    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:58.896773    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:58.897058    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:58.897072    5161 kubelet_node_status.go:379] Unable to update node status: update node status exceeds retry count
E0108 20:28:58.897483    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:28:58.971479    5161 kuberuntime_manager.go:513] Container {Name:api Image:openshift/origin-hypershift:v3.11.0 Command:[/bin/bash -c] Args:[#!/bin/bash
set -euo pipefail
if [[ -f /etc/origin/master/master.env ]]; then
  set -o allexport
  source /etc/origin/master/master.env
fi
exec hypershift openshift-kube-apiserver --config=/etc/origin/master/master-config.yaml
] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:master-data ReadOnly:false MountPath:/var/lib/origin/ SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:healthz,Port:8443,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:28:58.971583    5161 kuberuntime_manager.go:757] checking backoff for container "api" in pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)"
I0108 20:28:58.971691    5161 kuberuntime_manager.go:767] Back-off 10s restarting failed container=api pod=master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)
E0108 20:28:58.971745    5161 pod_workers.go:186] Error syncing pod 29e68324ed097a2c36aa5709e9b67154 ("master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)"), skipping: failed to "StartContainer" for "api" with CrashLoopBackOff: "Back-off 10s restarting failed container=api pod=master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)"
W0108 20:28:59.703068    5161 status_manager.go:482] Failed to get status for pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:59.897829    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:59.897909    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:28:59.905411    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:00.003468    5161 kuberuntime_manager.go:513] Container {Name:controllers Image:openshift/origin-hyperkube:v3.11.0 Command:[hyperkube kube-controller-manager] Args:[--enable-dynamic-provisioning=true --use-service-account-credentials=true --leader-elect-retry-period=3s --leader-elect-resource-lock=configmaps --controllers=* --controllers=-ttl --controllers=-bootstrapsigner --controllers=-tokencleaner --controllers=-horizontalpodautoscaling --pod-eviction-timeout=5m --cluster-signing-key-file= --cluster-signing-cert-file= --experimental-cluster-signing-duration=720h --root-ca-file=/etc/origin/master/ca-bundle.crt --port=10252 --service-account-private-key-file=/etc/origin/master/serviceaccounts.private.key --kubeconfig=/etc/origin/master/openshift-master.kubeconfig --openshift-config=/etc/origin/master/master-config.yaml] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:healthz,Port:10252,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:00.003589    5161 kuberuntime_manager.go:757] checking backoff for container "controllers" in pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)"
I0108 20:29:00.003694    5161 kuberuntime_manager.go:767] Back-off 10s restarting failed container=controllers pod=kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)
E0108 20:29:00.003727    5161 pod_workers.go:186] Error syncing pod dfcadfa6552711112062fbf1121a691c ("kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)"), skipping: failed to "StartContainer" for "controllers" with CrashLoopBackOff: "Back-off 10s restarting failed container=controllers pod=kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)"
E0108 20:29:00.177484    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/openshift-web-console/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
W0108 20:29:00.742038    5161 status_manager.go:482] Failed to get status for pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-apiserver/pods/openshift-apiserver-6q9ps: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:00.751279    5161 status_manager.go:482] Failed to get status for pod "persistent-volume-setup-5mzds_default(3567cf6f-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/default/pods/persistent-volume-setup-5mzds: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:00.881497    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "pvinstaller-token-zgfb9" (UniqueName: "kubernetes.io/secret/3567cf6f-3255-11ea-b26d-525400ca0151-pvinstaller-token-zgfb9") pod "3567cf6f-3255-11ea-b26d-525400ca0151" (UID: "3567cf6f-3255-11ea-b26d-525400ca0151") 
I0108 20:29:00.881553    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "pvdir" (UniqueName: "kubernetes.io/host-path/3567cf6f-3255-11ea-b26d-525400ca0151-pvdir") pod "3567cf6f-3255-11ea-b26d-525400ca0151" (UID: "3567cf6f-3255-11ea-b26d-525400ca0151") 
I0108 20:29:00.881669    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/3567cf6f-3255-11ea-b26d-525400ca0151-pvdir" (OuterVolumeSpecName: "pvdir") pod "3567cf6f-3255-11ea-b26d-525400ca0151" (UID: "3567cf6f-3255-11ea-b26d-525400ca0151"). InnerVolumeSpecName "pvdir". PluginName "kubernetes.io/host-path", VolumeGidValue ""
E0108 20:29:00.898966    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:00.908417    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:00.915790    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/3567cf6f-3255-11ea-b26d-525400ca0151-pvinstaller-token-zgfb9" (OuterVolumeSpecName: "pvinstaller-token-zgfb9") pod "3567cf6f-3255-11ea-b26d-525400ca0151" (UID: "3567cf6f-3255-11ea-b26d-525400ca0151"). InnerVolumeSpecName "pvinstaller-token-zgfb9". PluginName "kubernetes.io/secret", VolumeGidValue ""
E0108 20:29:00.915962    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:00.981817    5161 reconciler.go:301] Volume detached for volume "pvdir" (UniqueName: "kubernetes.io/host-path/3567cf6f-3255-11ea-b26d-525400ca0151-pvdir") on node "localhost" DevicePath ""
I0108 20:29:00.981847    5161 reconciler.go:301] Volume detached for volume "pvinstaller-token-zgfb9" (UniqueName: "kubernetes.io/secret/3567cf6f-3255-11ea-b26d-525400ca0151-pvinstaller-token-zgfb9") on node "localhost" DevicePath ""
W0108 20:29:01.780734    5161 pod_container_deletor.go:75] Container "0431875f86073a1df2c3f4c1dca7e3510b509a4d58d90618903dca1b69971b56" not found in pod's containers
W0108 20:29:01.792932    5161 status_manager.go:482] Failed to get status for pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-apiserver/pods/openshift-apiserver-6q9ps: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:01.899547    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:01.909074    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:01.916533    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:02.092639    5161 kuberuntime_manager.go:513] Container {Name:apiserver Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift openshift-apiserver] Args:[--config=/etc/origin/master/master-config.yaml -v=0] WorkingDir: Ports:[{Name: HostPort:8445 ContainerPort:8445 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:ADDITIONAL_ALLOWED_REGISTRIES Value:registry.centos.org ValueFrom:nil}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:serving-cert ReadOnly:false MountPath:/var/serving-cert SubPath: MountPropagation:<nil>} {Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:openshift-apiserver-token-dg6ph ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:8445,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:02.092783    5161 kuberuntime_manager.go:757] checking backoff for container "apiserver" in pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"
I0108 20:29:02.092938    5161 kuberuntime_manager.go:767] Back-off 10s restarting failed container=apiserver pod=openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)
E0108 20:29:02.092977    5161 pod_workers.go:186] Error syncing pod e3681100-3254-11ea-b26d-525400ca0151 ("openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"), skipping: failed to "StartContainer" for "apiserver" with CrashLoopBackOff: "Back-off 10s restarting failed container=apiserver pod=openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"
E0108 20:29:02.900203    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:02.910877    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:02.917206    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:03.900913    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:03.911468    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:03.917802    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:04.531572    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:04.582898    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:04.583222    5161 status_manager.go:482] Failed to get status for pod "persistent-volume-setup-5mzds_default(3567cf6f-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/default/pods/persistent-volume-setup-5mzds: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:04.583486    5161 status_manager.go:482] Failed to get status for pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:04.583737    5161 status_manager.go:482] Failed to get status for pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-apiserver/pods/openshift-apiserver-6q9ps: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:04.832049    5161 kuberuntime_manager.go:513] Container {Name:api Image:openshift/origin-hypershift:v3.11.0 Command:[/bin/bash -c] Args:[#!/bin/bash
set -euo pipefail
if [[ -f /etc/origin/master/master.env ]]; then
  set -o allexport
  source /etc/origin/master/master.env
fi
exec hypershift openshift-kube-apiserver --config=/etc/origin/master/master-config.yaml
] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:master-data ReadOnly:false MountPath:/var/lib/origin/ SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:healthz,Port:8443,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:04.832168    5161 kuberuntime_manager.go:757] checking backoff for container "api" in pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)"
I0108 20:29:04.832275    5161 kuberuntime_manager.go:767] Back-off 10s restarting failed container=api pod=master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)
E0108 20:29:04.832306    5161 pod_workers.go:186] Error syncing pod 29e68324ed097a2c36aa5709e9b67154 ("master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)"), skipping: failed to "StartContainer" for "api" with CrashLoopBackOff: "Back-off 10s restarting failed container=api pod=master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)"
E0108 20:29:04.901605    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:04.912110    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:04.918504    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:05.854701    5161 status_manager.go:482] Failed to get status for pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/pods/openshift-controller-manager-cp9j7: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:05.902339    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:05.913188    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:05.919266    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:06.006321    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:06.006450    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:29:06.506400989 +0000 UTC m=+372.216184953 (durationBeforeRetry 500ms). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
I0108 20:29:06.154581    5161 kuberuntime_manager.go:513] Container {Name:c Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift openshift-controller-manager] Args:[--config=/etc/origin/master/master-config.yaml --v=0] WorkingDir: Ports:[{Name: HostPort:8444 ContainerPort:8444 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:openshift-controller-manager-token-t74x8 ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:8444,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:06.154744    5161 kuberuntime_manager.go:757] checking backoff for container "c" in pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)"
I0108 20:29:06.154925    5161 kuberuntime_manager.go:767] Back-off 10s restarting failed container=c pod=openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)
E0108 20:29:06.154973    5161 pod_workers.go:186] Error syncing pod 3334e363-3255-11ea-b26d-525400ca0151 ("openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)"), skipping: failed to "StartContainer" for "c" with CrashLoopBackOff: "Back-off 10s restarting failed container=c pod=openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)"
E0108 20:29:06.507861    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:06.508004    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:29:07.50794785 +0000 UTC m=+373.217731812 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:29:06.903120    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:06.913880    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:06.919882    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:07.173433    5161 status_manager.go:482] Failed to get status for pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:07.473411    5161 kuberuntime_manager.go:513] Container {Name:controllers Image:openshift/origin-hyperkube:v3.11.0 Command:[hyperkube kube-controller-manager] Args:[--enable-dynamic-provisioning=true --use-service-account-credentials=true --leader-elect-retry-period=3s --leader-elect-resource-lock=configmaps --controllers=* --controllers=-ttl --controllers=-bootstrapsigner --controllers=-tokencleaner --controllers=-horizontalpodautoscaling --pod-eviction-timeout=5m --cluster-signing-key-file= --cluster-signing-cert-file= --experimental-cluster-signing-duration=720h --root-ca-file=/etc/origin/master/ca-bundle.crt --port=10252 --service-account-private-key-file=/etc/origin/master/serviceaccounts.private.key --kubeconfig=/etc/origin/master/openshift-master.kubeconfig --openshift-config=/etc/origin/master/master-config.yaml] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:healthz,Port:10252,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:07.473524    5161 kuberuntime_manager.go:757] checking backoff for container "controllers" in pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)"
I0108 20:29:07.473633    5161 kuberuntime_manager.go:767] Back-off 10s restarting failed container=controllers pod=kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)
E0108 20:29:07.473665    5161 pod_workers.go:186] Error syncing pod dfcadfa6552711112062fbf1121a691c ("kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)"), skipping: failed to "StartContainer" for "controllers" with CrashLoopBackOff: "Back-off 10s restarting failed container=controllers pod=kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)"
E0108 20:29:07.510969    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:07.511095    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:29:09.511047359 +0000 UTC m=+375.220831329 (durationBeforeRetry 2s). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:29:07.905078    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:07.915353    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:07.921389    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:08.901065    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?resourceVersion=0&timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:08.901992    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:08.903159    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:08.903537    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:08.903884    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:08.903897    5161 kubelet_node_status.go:379] Unable to update node status: update node status exceeds retry count
E0108 20:29:08.908253    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:08.916006    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:08.922026    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:09.518035    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:09.518143    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:29:13.518104454 +0000 UTC m=+379.227888413 (durationBeforeRetry 4s). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:29:09.909399    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:09.917431    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:09.922631    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:10.178222    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/openshift-web-console/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
E0108 20:29:10.910139    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:10.918075    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:10.926067    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:11.910918    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:11.918743    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:11.933547    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:12.911650    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:12.919370    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:12.934123    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:13.540869    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:13.540991    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:29:21.540953758 +0000 UTC m=+387.250737718 (durationBeforeRetry 8s). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
W0108 20:29:13.582633    5161 status_manager.go:482] Failed to get status for pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-apiserver/pods/openshift-apiserver-6q9ps: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:13.882653    5161 kuberuntime_manager.go:513] Container {Name:apiserver Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift openshift-apiserver] Args:[--config=/etc/origin/master/master-config.yaml -v=0] WorkingDir: Ports:[{Name: HostPort:8445 ContainerPort:8445 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:ADDITIONAL_ALLOWED_REGISTRIES Value:registry.centos.org ValueFrom:nil}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:serving-cert ReadOnly:false MountPath:/var/serving-cert SubPath: MountPropagation:<nil>} {Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:openshift-apiserver-token-dg6ph ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:8445,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:13.882746    5161 kuberuntime_manager.go:757] checking backoff for container "apiserver" in pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"
E0108 20:29:13.912322    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:13.920035    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:13.934704    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:14.582896    5161 status_manager.go:482] Failed to get status for pod "persistent-volume-setup-5mzds_default(3567cf6f-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/default/pods/persistent-volume-setup-5mzds: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:14.583200    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:14.583524    5161 status_manager.go:482] Failed to get status for pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/pods/openshift-controller-manager-cp9j7: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:14.583780    5161 status_manager.go:482] Failed to get status for pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-apiserver/pods/openshift-apiserver-6q9ps: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:14.584068    5161 status_manager.go:482] Failed to get status for pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:14.913165    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:14.920730    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:14.935308    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:15.913935    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:15.922071    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:15.935889    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:16.914647    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:16.923749    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:16.936534    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:17.034583    5161 status_manager.go:482] Failed to get status for pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-apiserver/pods/openshift-apiserver-6q9ps: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:17.593246    5161 status_manager.go:482] Failed to get status for pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/pods/openshift-controller-manager-cp9j7: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:17.882555    5161 kuberuntime_manager.go:513] Container {Name:c Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift openshift-controller-manager] Args:[--config=/etc/origin/master/master-config.yaml --v=0] WorkingDir: Ports:[{Name: HostPort:8444 ContainerPort:8444 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:openshift-controller-manager-token-t74x8 ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:8444,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:17.882704    5161 kuberuntime_manager.go:757] checking backoff for container "c" in pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)"
E0108 20:29:17.915444    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:17.924350    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:17.937200    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:18.056778    5161 status_manager.go:482] Failed to get status for pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-apiserver/pods/openshift-apiserver-6q9ps: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:18.356496    5161 kuberuntime_manager.go:513] Container {Name:apiserver Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift openshift-apiserver] Args:[--config=/etc/origin/master/master-config.yaml -v=0] WorkingDir: Ports:[{Name: HostPort:8445 ContainerPort:8445 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:ADDITIONAL_ALLOWED_REGISTRIES Value:registry.centos.org ValueFrom:nil}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:serving-cert ReadOnly:false MountPath:/var/serving-cert SubPath: MountPropagation:<nil>} {Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:openshift-apiserver-token-dg6ph ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:8445,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:18.356678    5161 kuberuntime_manager.go:757] checking backoff for container "apiserver" in pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"
I0108 20:29:18.356849    5161 kuberuntime_manager.go:767] Back-off 20s restarting failed container=apiserver pod=openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)
E0108 20:29:18.356895    5161 pod_workers.go:186] Error syncing pod e3681100-3254-11ea-b26d-525400ca0151 ("openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"), skipping: failed to "StartContainer" for "apiserver" with CrashLoopBackOff: "Back-off 20s restarting failed container=apiserver pod=openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"
I0108 20:29:18.882587    5161 kuberuntime_manager.go:513] Container {Name:api Image:openshift/origin-hypershift:v3.11.0 Command:[/bin/bash -c] Args:[#!/bin/bash
set -euo pipefail
if [[ -f /etc/origin/master/master.env ]]; then
  set -o allexport
  source /etc/origin/master/master.env
fi
exec hypershift openshift-kube-apiserver --config=/etc/origin/master/master-config.yaml
] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:master-data ReadOnly:false MountPath:/var/lib/origin/ SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:healthz,Port:8443,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:18.882727    5161 kuberuntime_manager.go:757] checking backoff for container "api" in pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)"
E0108 20:29:18.904737    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?resourceVersion=0&timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:18.905233    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:18.905566    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:18.905916    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:18.906197    5161 kubelet_node_status.go:391] Error updating node status, will retry: error getting node "localhost": Get https://localhost:8443/api/v1/nodes/localhost?timeout=10s: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:18.906210    5161 kubelet_node_status.go:379] Unable to update node status: update node status exceeds retry count
E0108 20:29:18.916152    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:18.925046    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:18.937875    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:19.916995    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:19.925657    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:19.938599    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:20.179039    5161 event.go:212] Unable to write event: 'Post https://localhost:8443/api/v1/namespaces/openshift-web-console/events: dial tcp 127.0.0.1:8443: connect: connection refused' (may retry after sleeping)
I0108 20:29:20.889364    5161 kuberuntime_manager.go:513] Container {Name:controllers Image:openshift/origin-hyperkube:v3.11.0 Command:[hyperkube kube-controller-manager] Args:[--enable-dynamic-provisioning=true --use-service-account-credentials=true --leader-elect-retry-period=3s --leader-elect-resource-lock=configmaps --controllers=* --controllers=-ttl --controllers=-bootstrapsigner --controllers=-tokencleaner --controllers=-horizontalpodautoscaling --pod-eviction-timeout=5m --cluster-signing-key-file= --cluster-signing-cert-file= --experimental-cluster-signing-duration=720h --root-ca-file=/etc/origin/master/ca-bundle.crt --port=10252 --service-account-private-key-file=/etc/origin/master/serviceaccounts.private.key --kubeconfig=/etc/origin/master/openshift-master.kubeconfig --openshift-config=/etc/origin/master/master-config.yaml] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:healthz,Port:10252,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:20.889480    5161 kuberuntime_manager.go:757] checking backoff for container "controllers" in pod "kube-controller-manager-localhost_kube-system(dfcadfa6552711112062fbf1121a691c)"
E0108 20:29:20.920353    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:20.926554    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:20.939245    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:21.108850    5161 status_manager.go:482] Failed to get status for pod "openshift-controller-manager-cp9j7_openshift-controller-manager(3334e363-3255-11ea-b26d-525400ca0151)": Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/pods/openshift-controller-manager-cp9j7: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:21.610270    5161 secret.go:198] Couldn't get secret openshift-controller-manager/openshift-controller-manager-token-t74x8: Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:21.610367    5161 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\" (\"3334e363-3255-11ea-b26d-525400ca0151\")" failed. No retries permitted until 2020-01-08 20:29:37.610328638 +0000 UTC m=+403.320112598 (durationBeforeRetry 16s). Error: "MountVolume.SetUp failed for volume \"openshift-controller-manager-token-t74x8\" (UniqueName: \"kubernetes.io/secret/3334e363-3255-11ea-b26d-525400ca0151-openshift-controller-manager-token-t74x8\") pod \"openshift-controller-manager-cp9j7\" (UID: \"3334e363-3255-11ea-b26d-525400ca0151\") : Get https://localhost:8443/api/v1/namespaces/openshift-controller-manager/secrets/openshift-controller-manager-token-t74x8: dial tcp 127.0.0.1:8443: connect: connection refused"
E0108 20:29:21.921115    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:21.927189    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:21.939867    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:22.923454    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://localhost:8443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:22.930920    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:464: Failed to list *v1.Node: Get https://localhost:8443/api/v1/nodes?fieldSelector=metadata.name%3Dlocalhost&limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
E0108 20:29:22.945953    5161 reflector.go:136] k8s.io/kubernetes/pkg/kubelet/kubelet.go:455: Failed to list *v1.Service: Get https://localhost:8443/api/v1/services?limit=500&resourceVersion=0: dial tcp 127.0.0.1:8443: connect: connection refused
W0108 20:29:23.147048    5161 status_manager.go:482] Failed to get status for pod "master-api-localhost_kube-system(29e68324ed097a2c36aa5709e9b67154)": Get https://localhost:8443/api/v1/namespaces/kube-system/pods/master-api-localhost: dial tcp 127.0.0.1:8443: connect: connection refused
I0108 20:29:32.885197    5161 kuberuntime_manager.go:513] Container {Name:apiserver Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift openshift-apiserver] Args:[--config=/etc/origin/master/master-config.yaml -v=0] WorkingDir: Ports:[{Name: HostPort:8445 ContainerPort:8445 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:ADDITIONAL_ALLOWED_REGISTRIES Value:registry.centos.org ValueFrom:nil}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:serving-cert ReadOnly:false MountPath:/var/serving-cert SubPath: MountPropagation:<nil>} {Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:openshift-apiserver-token-dg6ph ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:8445,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:32.885319    5161 kuberuntime_manager.go:757] checking backoff for container "apiserver" in pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"
I0108 20:29:32.885451    5161 kuberuntime_manager.go:767] Back-off 20s restarting failed container=apiserver pod=openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)
E0108 20:29:32.885481    5161 pod_workers.go:186] Error syncing pod e3681100-3254-11ea-b26d-525400ca0151 ("openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"), skipping: failed to "StartContainer" for "apiserver" with CrashLoopBackOff: "Back-off 20s restarting failed container=apiserver pod=openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"
E0108 20:29:42.441294    5161 event.go:203] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"webconsole-85c9b79fdf-56nvm.15e8034a2acfb9d4", GenerateName:"", Namespace:"openshift-web-console", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"openshift-web-console", Name:"webconsole-85c9b79fdf-56nvm", UID:"601081ba-3255-11ea-b26d-525400ca0151", APIVersion:"v1", ResourceVersion:"2209", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"serving-cert\" : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-serving-cert: unexpected EOF", Source:v1.EventSource{Component:"kubelet", Host:"localhost"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb59c41ddd4, ext:312183862637, loc:(*time.Location)(0x90a9e60)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb59c41ddd4, ext:312183862637, loc:(*time.Location)(0x90a9e60)}}, Count:1, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "webconsole-85c9b79fdf-56nvm.15e8034a2acfb9d4" is forbidden: caches not synchronized' (will not retry!)
I0108 20:29:44.891378    5161 kuberuntime_manager.go:513] Container {Name:apiserver Image:openshift/origin-hypershift:v3.11.0 Command:[hypershift openshift-apiserver] Args:[--config=/etc/origin/master/master-config.yaml -v=0] WorkingDir: Ports:[{Name: HostPort:8445 ContainerPort:8445 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:ADDITIONAL_ALLOWED_REGISTRIES Value:registry.centos.org ValueFrom:nil}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:serving-cert ReadOnly:false MountPath:/var/serving-cert SubPath: MountPropagation:<nil>} {Name:master-config ReadOnly:false MountPath:/etc/origin/master/ SubPath: MountPropagation:<nil>} {Name:master-cloud-provider ReadOnly:false MountPath:/etc/origin/cloudprovider/ SubPath: MountPropagation:<nil>} {Name:openshift-apiserver-token-dg6ph ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:8445,Host:,Scheme:HTTPS,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:Always SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
I0108 20:29:44.891611    5161 kuberuntime_manager.go:757] checking backoff for container "apiserver" in pod "openshift-apiserver-6q9ps_openshift-apiserver(e3681100-3254-11ea-b26d-525400ca0151)"
E0108 20:29:46.909749    5161 fsHandler.go:121] failed to collect filesystem stats - rootDiskErr: du command failed on /rootfs/var/lib/docker/overlay2/9bf4e5f09bb371d003cd87d90cb071df7d8aa4e916843d299bdb7f1af7955015/diff with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/overlay2/9bf4e5f09bb371d003cd87d90cb071df7d8aa4e916843d299bdb7f1af7955015/diff': No such file or directory
 - exit status 1, rootInodeErr: cmd [ionice -c3 nice -n 19 find /rootfs/var/lib/docker/overlay2/9bf4e5f09bb371d003cd87d90cb071df7d8aa4e916843d299bdb7f1af7955015/diff -xdev -printf .] failed. stderr: find: '/rootfs/var/lib/docker/overlay2/9bf4e5f09bb371d003cd87d90cb071df7d8aa4e916843d299bdb7f1af7955015/diff': No such file or directory
; err: exit status 1, extraDiskErr: du command failed on /rootfs/var/lib/docker/containers/86830f935d1d57650e1b1992626083583ca5a8e1629ca63befce6e195258de4f with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/containers/86830f935d1d57650e1b1992626083583ca5a8e1629ca63befce6e195258de4f': No such file or directory
 - exit status 1
E0108 20:29:52.449069    5161 event.go:203] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"webconsole-85c9b79fdf-56nvm.15e8034a2ad13e24", GenerateName:"", Namespace:"openshift-web-console", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"openshift-web-console", Name:"webconsole-85c9b79fdf-56nvm", UID:"601081ba-3255-11ea-b26d-525400ca0151", APIVersion:"v1", ResourceVersion:"2209", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"webconsole-config\" : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: unexpected EOF", Source:v1.EventSource{Component:"kubelet", Host:"localhost"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb59c436224, ext:312183962040, loc:(*time.Location)(0x90a9e60)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb59c436224, ext:312183962040, loc:(*time.Location)(0x90a9e60)}}, Count:1, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "webconsole-85c9b79fdf-56nvm.15e8034a2ad13e24" is forbidden: caches not synchronized' (will not retry!)
E0108 20:30:02.461020    5161 event.go:203] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"webconsole-85c9b79fdf-56nvm.15e8034a4cab8f77", GenerateName:"", Namespace:"openshift-web-console", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"openshift-web-console", Name:"webconsole-85c9b79fdf-56nvm", UID:"601081ba-3255-11ea-b26d-525400ca0151", APIVersion:"v1", ResourceVersion:"2209", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"webconsole-token-mhn7m\" : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/secrets/webconsole-token-mhn7m: dial tcp 127.0.0.1:8443: connect: connection refused", Source:v1.EventSource{Component:"kubelet", Host:"localhost"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb5c282e977, ext:312751917848, loc:(*time.Location)(0x90a9e60)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb5c282e977, ext:312751917848, loc:(*time.Location)(0x90a9e60)}}, Count:1, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "webconsole-85c9b79fdf-56nvm.15e8034a4cab8f77" is forbidden: caches not synchronized' (will not retry!)
I0108 20:30:05.871003    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "pvinstaller-token-zgfb9" (UniqueName: "kubernetes.io/secret/a7a020dd-3255-11ea-93c0-525400ca0151-pvinstaller-token-zgfb9") pod "persistent-volume-setup-rsqjw" (UID: "a7a020dd-3255-11ea-93c0-525400ca0151") 
I0108 20:30:05.871067    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "pvdir" (UniqueName: "kubernetes.io/host-path/a7a020dd-3255-11ea-93c0-525400ca0151-pvdir") pod "persistent-volume-setup-rsqjw" (UID: "a7a020dd-3255-11ea-93c0-525400ca0151") 
W0108 20:30:09.283992    5161 pod_container_deletor.go:75] Container "ceb2ac2047dfb279fd6097c44e4a01515e23c86357601c40bfa83a90f144c74c" not found in pod's containers
E0108 20:30:12.500345    5161 event.go:203] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"webconsole-85c9b79fdf-56nvm.15e8034a4cad9e2e", GenerateName:"", Namespace:"openshift-web-console", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"openshift-web-console", Name:"webconsole-85c9b79fdf-56nvm", UID:"601081ba-3255-11ea-b26d-525400ca0151", APIVersion:"v1", ResourceVersion:"2209", FieldPath:""}, Reason:"FailedMount", Message:"MountVolume.SetUp failed for volume \"webconsole-config\" : Get https://localhost:8443/api/v1/namespaces/openshift-web-console/configmaps/webconsole-config: dial tcp 127.0.0.1:8443: connect: connection refused", Source:v1.EventSource{Component:"kubelet", Host:"localhost"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb5c284f82e, ext:312752052678, loc:(*time.Location)(0x90a9e60)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xbf7dacb5c284f82e, ext:312752052678, loc:(*time.Location)(0x90a9e60)}}, Count:1, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'events "webconsole-85c9b79fdf-56nvm.15e8034a4cad9e2e" is forbidden: caches not synchronized' (will not retry!)
I0108 20:30:13.438987    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "registry-token-b8dwv" (UniqueName: "kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv") pod "4ef4964d-3255-11ea-b26d-525400ca0151" (UID: "4ef4964d-3255-11ea-b26d-525400ca0151") 
I0108 20:30:13.439677    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "registry-storage" (UniqueName: "kubernetes.io/host-path/4ef4964d-3255-11ea-b26d-525400ca0151-registry-storage") pod "4ef4964d-3255-11ea-b26d-525400ca0151" (UID: "4ef4964d-3255-11ea-b26d-525400ca0151") 
I0108 20:30:13.439794    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/4ef4964d-3255-11ea-b26d-525400ca0151-registry-storage" (OuterVolumeSpecName: "registry-storage") pod "4ef4964d-3255-11ea-b26d-525400ca0151" (UID: "4ef4964d-3255-11ea-b26d-525400ca0151"). InnerVolumeSpecName "registry-storage". PluginName "kubernetes.io/host-path", VolumeGidValue ""
I0108 20:30:13.494867    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv" (OuterVolumeSpecName: "registry-token-b8dwv") pod "4ef4964d-3255-11ea-b26d-525400ca0151" (UID: "4ef4964d-3255-11ea-b26d-525400ca0151"). InnerVolumeSpecName "registry-token-b8dwv". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0108 20:30:13.542945    5161 reconciler.go:301] Volume detached for volume "registry-token-b8dwv" (UniqueName: "kubernetes.io/secret/4ef4964d-3255-11ea-b26d-525400ca0151-registry-token-b8dwv") on node "localhost" DevicePath ""
I0108 20:30:13.542978    5161 reconciler.go:301] Volume detached for volume "registry-storage" (UniqueName: "kubernetes.io/host-path/4ef4964d-3255-11ea-b26d-525400ca0151-registry-storage") on node "localhost" DevicePath ""
E0108 20:30:41.524972    5161 fsHandler.go:121] failed to collect filesystem stats - rootDiskErr: du command failed on /rootfs/var/lib/docker/overlay2/47862618f433eecdcbf51495f269d669c1299ef0749762ef3d016472236f0105/diff with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/overlay2/47862618f433eecdcbf51495f269d669c1299ef0749762ef3d016472236f0105/diff': No such file or directory
 - exit status 1, rootInodeErr: cmd [ionice -c3 nice -n 19 find /rootfs/var/lib/docker/overlay2/47862618f433eecdcbf51495f269d669c1299ef0749762ef3d016472236f0105/diff -xdev -printf .] failed. stderr: find: '/rootfs/var/lib/docker/overlay2/47862618f433eecdcbf51495f269d669c1299ef0749762ef3d016472236f0105/diff': No such file or directory
; err: exit status 1, extraDiskErr: du command failed on /rootfs/var/lib/docker/containers/4f6fb72c066a516f9f318de90182a551a4e6d275a088472e8ddbe4d66aec3411 with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/containers/4f6fb72c066a516f9f318de90182a551a4e6d275a088472e8ddbe4d66aec3411': No such file or directory
 - exit status 1
I0108 20:31:44.628542    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "pvdir" (UniqueName: "kubernetes.io/host-path/a7a020dd-3255-11ea-93c0-525400ca0151-pvdir") pod "a7a020dd-3255-11ea-93c0-525400ca0151" (UID: "a7a020dd-3255-11ea-93c0-525400ca0151") 
I0108 20:31:44.628604    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "pvinstaller-token-zgfb9" (UniqueName: "kubernetes.io/secret/a7a020dd-3255-11ea-93c0-525400ca0151-pvinstaller-token-zgfb9") pod "a7a020dd-3255-11ea-93c0-525400ca0151" (UID: "a7a020dd-3255-11ea-93c0-525400ca0151") 
I0108 20:31:44.646723    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/a7a020dd-3255-11ea-93c0-525400ca0151-pvdir" (OuterVolumeSpecName: "pvdir") pod "a7a020dd-3255-11ea-93c0-525400ca0151" (UID: "a7a020dd-3255-11ea-93c0-525400ca0151"). InnerVolumeSpecName "pvdir". PluginName "kubernetes.io/host-path", VolumeGidValue ""
I0108 20:31:44.729066    5161 reconciler.go:301] Volume detached for volume "pvdir" (UniqueName: "kubernetes.io/host-path/a7a020dd-3255-11ea-93c0-525400ca0151-pvdir") on node "localhost" DevicePath ""
I0108 20:31:44.795262    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/a7a020dd-3255-11ea-93c0-525400ca0151-pvinstaller-token-zgfb9" (OuterVolumeSpecName: "pvinstaller-token-zgfb9") pod "a7a020dd-3255-11ea-93c0-525400ca0151" (UID: "a7a020dd-3255-11ea-93c0-525400ca0151"). InnerVolumeSpecName "pvinstaller-token-zgfb9". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0108 20:31:44.829330    5161 reconciler.go:301] Volume detached for volume "pvinstaller-token-zgfb9" (UniqueName: "kubernetes.io/secret/a7a020dd-3255-11ea-93c0-525400ca0151-pvinstaller-token-zgfb9") on node "localhost" DevicePath ""
W0108 20:31:45.891550    5161 pod_container_deletor.go:75] Container "ceb2ac2047dfb279fd6097c44e4a01515e23c86357601c40bfa83a90f144c74c" not found in pod's containers
I0108 21:28:54.101247    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "deployer-token-kdk2h" (UniqueName: "kubernetes.io/secret/dda50dcf-325d-11ea-93c0-525400ca0151-deployer-token-kdk2h") pod "docker-registry-2-deploy" (UID: "dda50dcf-325d-11ea-93c0-525400ca0151") 
I0108 21:28:54.321147    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "deployer-token-kdk2h" (UniqueName: "kubernetes.io/secret/de102ecf-325d-11ea-93c0-525400ca0151-deployer-token-kdk2h") pod "router-2-deploy" (UID: "de102ecf-325d-11ea-93c0-525400ca0151") 
I0108 21:29:02.173541    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "registry-storage" (UniqueName: "kubernetes.io/host-path/e382b9fe-325d-11ea-93c0-525400ca0151-registry-storage") pod "docker-registry-2-9zjpd" (UID: "e382b9fe-325d-11ea-93c0-525400ca0151") 
I0108 21:29:02.173591    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "registry-token-b8dwv" (UniqueName: "kubernetes.io/secret/e382b9fe-325d-11ea-93c0-525400ca0151-registry-token-b8dwv") pod "docker-registry-2-9zjpd" (UID: "e382b9fe-325d-11ea-93c0-525400ca0151") 
I0108 21:29:02.385003    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "server-certificate" (UniqueName: "kubernetes.io/secret/e3829e15-325d-11ea-93c0-525400ca0151-server-certificate") pod "router-2-hbbc6" (UID: "e3829e15-325d-11ea-93c0-525400ca0151") 
I0108 21:29:02.385066    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "metrics-server-certificate" (UniqueName: "kubernetes.io/secret/e3829e15-325d-11ea-93c0-525400ca0151-metrics-server-certificate") pod "router-2-hbbc6" (UID: "e3829e15-325d-11ea-93c0-525400ca0151") 
I0108 21:29:02.385097    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "router-token-t2dkw" (UniqueName: "kubernetes.io/secret/e3829e15-325d-11ea-93c0-525400ca0151-router-token-t2dkw") pod "router-2-hbbc6" (UID: "e3829e15-325d-11ea-93c0-525400ca0151") 
I0108 21:29:12.459090    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "deployer-token-kdk2h" (UniqueName: "kubernetes.io/secret/dda50dcf-325d-11ea-93c0-525400ca0151-deployer-token-kdk2h") pod "dda50dcf-325d-11ea-93c0-525400ca0151" (UID: "dda50dcf-325d-11ea-93c0-525400ca0151") 
I0108 21:29:12.489243    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/dda50dcf-325d-11ea-93c0-525400ca0151-deployer-token-kdk2h" (OuterVolumeSpecName: "deployer-token-kdk2h") pod "dda50dcf-325d-11ea-93c0-525400ca0151" (UID: "dda50dcf-325d-11ea-93c0-525400ca0151"). InnerVolumeSpecName "deployer-token-kdk2h". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0108 21:29:12.560800    5161 reconciler.go:301] Volume detached for volume "deployer-token-kdk2h" (UniqueName: "kubernetes.io/secret/dda50dcf-325d-11ea-93c0-525400ca0151-deployer-token-kdk2h") on node "localhost" DevicePath ""
W0108 21:29:13.985644    5161 pod_container_deletor.go:75] Container "622b12d9feb68c7058a83716c4550666e38cffc4cb90420109e881dc902b1b20" not found in pod's containers
I0108 21:29:25.331780    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "deployer-token-kdk2h" (UniqueName: "kubernetes.io/secret/de102ecf-325d-11ea-93c0-525400ca0151-deployer-token-kdk2h") pod "de102ecf-325d-11ea-93c0-525400ca0151" (UID: "de102ecf-325d-11ea-93c0-525400ca0151") 
I0108 21:29:25.359589    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/de102ecf-325d-11ea-93c0-525400ca0151-deployer-token-kdk2h" (OuterVolumeSpecName: "deployer-token-kdk2h") pod "de102ecf-325d-11ea-93c0-525400ca0151" (UID: "de102ecf-325d-11ea-93c0-525400ca0151"). InnerVolumeSpecName "deployer-token-kdk2h". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0108 21:29:25.432191    5161 reconciler.go:301] Volume detached for volume "deployer-token-kdk2h" (UniqueName: "kubernetes.io/secret/de102ecf-325d-11ea-93c0-525400ca0151-deployer-token-kdk2h") on node "localhost" DevicePath ""
W0108 21:29:26.229841    5161 pod_container_deletor.go:75] Container "c97993c7d1b17c388ff24302e3e99a57a8c042358f1a9d59d94d96bcd07d6c1d" not found in pod's containers
W0108 21:29:26.677418    5161 kubelet_getters.go:272] Path "/var/lib/minishift/base/openshift.local.volumes/pods/de102ecf-325d-11ea-93c0-525400ca0151/volumes" does not exist
E0108 21:30:44.380978    5161 file_linux.go:112] Not recursing into manifest path "/var/lib/origin/pod-manifests"
I0108 21:33:44.029859    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "vault-auth-token-4zfg6" (UniqueName: "kubernetes.io/secret/8b7f261e-325e-11ea-93c0-525400ca0151-vault-auth-token-4zfg6") pod "vault-56b856d486-h7l7w" (UID: "8b7f261e-325e-11ea-93c0-525400ca0151") 
I0108 21:54:46.154527    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "vault-injector-token-sn2x7" (UniqueName: "kubernetes.io/secret/7be5fa10-3261-11ea-93c0-525400ca0151-vault-injector-token-sn2x7") pod "vault-injector-7d4bb88bd7-snsjc" (UID: "7be5fa10-3261-11ea-93c0-525400ca0151") 
I0108 22:02:45.800858    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "pv0077" (UniqueName: "kubernetes.io/host-path/9936e220-3262-11ea-93c0-525400ca0151-pv0077") pod "app1-6499f99cb9-nmjpm" (UID: "9936e220-3262-11ea-93c0-525400ca0151") 
I0108 22:02:45.800932    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "app1-token-k9r8f" (UniqueName: "kubernetes.io/secret/9936e220-3262-11ea-93c0-525400ca0151-app1-token-k9r8f") pod "app1-6499f99cb9-nmjpm" (UID: "9936e220-3262-11ea-93c0-525400ca0151") 
E0108 22:02:45.980461    5161 helpers.go:135] readString: Failed to read "/rootfs/sys/fs/cgroup/memory/system.slice/run-18979.scope/memory.limit_in_bytes": read /rootfs/sys/fs/cgroup/memory/system.slice/run-18979.scope/memory.limit_in_bytes: no such device
E0108 22:02:45.980553    5161 helpers.go:135] readString: Failed to read "/rootfs/sys/fs/cgroup/memory/system.slice/run-18979.scope/memory.memsw.limit_in_bytes": read /rootfs/sys/fs/cgroup/memory/system.slice/run-18979.scope/memory.memsw.limit_in_bytes: no such device
E0108 22:02:45.980573    5161 helpers.go:135] readString: Failed to read "/rootfs/sys/fs/cgroup/memory/system.slice/run-18979.scope/memory.soft_limit_in_bytes": read /rootfs/sys/fs/cgroup/memory/system.slice/run-18979.scope/memory.soft_limit_in_bytes: no such device
E0108 22:02:45.980711    5161 helpers.go:135] readString: Failed to read "/rootfs/sys/fs/cgroup/memory/system.slice/run-18979.scope/memory.limit_in_bytes": read /rootfs/sys/fs/cgroup/memory/system.slice/run-18979.scope/memory.limit_in_bytes: no such device
E0108 22:02:45.980733    5161 helpers.go:135] readString: Failed to read "/rootfs/sys/fs/cgroup/memory/system.slice/run-18979.scope/memory.memsw.limit_in_bytes": read /rootfs/sys/fs/cgroup/memory/system.slice/run-18979.scope/memory.memsw.limit_in_bytes: no such device
E0108 22:02:45.980751    5161 helpers.go:135] readString: Failed to read "/rootfs/sys/fs/cgroup/memory/system.slice/run-18979.scope/memory.soft_limit_in_bytes": read /rootfs/sys/fs/cgroup/memory/system.slice/run-18979.scope/memory.soft_limit_in_bytes: no such device
W0108 22:02:45.989260    5161 container.go:507] Failed to update stats for container "/system.slice/run-18979.scope": failed to parse memory.usage_in_bytes - read /rootfs/sys/fs/cgroup/memory/system.slice/run-18979.scope/memory.usage_in_bytes: no such device, continuing to push stats
E0108 22:02:48.900118    5161 cadvisor_stats_provider.go:370] Partial failure issuing cadvisor.ContainerInfoV2: partial failures: ["/system.slice/run-18979.scope": RecentStats: unable to find data for container /system.slice/run-18979.scope]
I0108 22:04:11.179550    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "app1-token-k9r8f" (UniqueName: "kubernetes.io/secret/cc8b89d2-3262-11ea-93c0-525400ca0151-app1-token-k9r8f") pod "app1-566b66f668-jlpqs" (UID: "cc8b89d2-3262-11ea-93c0-525400ca0151") 
I0108 22:04:11.179708    5161 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume "pv0077" (UniqueName: "kubernetes.io/host-path/cc8b89d2-3262-11ea-93c0-525400ca0151-pv0077") pod "app1-566b66f668-jlpqs" (UID: "cc8b89d2-3262-11ea-93c0-525400ca0151") 
E0108 22:04:15.247522    5161 remote_runtime.go:278] ContainerStatus "632a79346ed0d27c4040d16cca78204843f66a5dcd7f8228a19b4b68e00d373d" from runtime service failed: rpc error: code = Unknown desc = Error: No such container: 632a79346ed0d27c4040d16cca78204843f66a5dcd7f8228a19b4b68e00d373d
I0108 22:04:15.375169    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "app1-token-k9r8f" (UniqueName: "kubernetes.io/secret/9936e220-3262-11ea-93c0-525400ca0151-app1-token-k9r8f") pod "9936e220-3262-11ea-93c0-525400ca0151" (UID: "9936e220-3262-11ea-93c0-525400ca0151") 
I0108 22:04:15.375641    5161 reconciler.go:181] operationExecutor.UnmountVolume started for volume "mypvc" (UniqueName: "kubernetes.io/host-path/9936e220-3262-11ea-93c0-525400ca0151-pv0077") pod "9936e220-3262-11ea-93c0-525400ca0151" (UID: "9936e220-3262-11ea-93c0-525400ca0151") 
I0108 22:04:15.379231    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/9936e220-3262-11ea-93c0-525400ca0151-pv0077" (OuterVolumeSpecName: "mypvc") pod "9936e220-3262-11ea-93c0-525400ca0151" (UID: "9936e220-3262-11ea-93c0-525400ca0151"). InnerVolumeSpecName "pv0077". PluginName "kubernetes.io/host-path", VolumeGidValue ""
I0108 22:04:15.379386    5161 reconciler.go:301] Volume detached for volume "pv0077" (UniqueName: "kubernetes.io/host-path/9936e220-3262-11ea-93c0-525400ca0151-pv0077") on node "localhost" DevicePath ""
I0108 22:04:15.423594    5161 operation_generator.go:688] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/9936e220-3262-11ea-93c0-525400ca0151-app1-token-k9r8f" (OuterVolumeSpecName: "app1-token-k9r8f") pod "9936e220-3262-11ea-93c0-525400ca0151" (UID: "9936e220-3262-11ea-93c0-525400ca0151"). InnerVolumeSpecName "app1-token-k9r8f". PluginName "kubernetes.io/secret", VolumeGidValue ""
I0108 22:04:15.479788    5161 reconciler.go:301] Volume detached for volume "app1-token-k9r8f" (UniqueName: "kubernetes.io/secret/9936e220-3262-11ea-93c0-525400ca0151-app1-token-k9r8f") on node "localhost" DevicePath ""
E0108 22:04:48.222497    5161 fsHandler.go:121] failed to collect filesystem stats - rootDiskErr: du command failed on /rootfs/var/lib/docker/overlay2/d414e8b5f99c94ce4912e0eea3f46e87e65a9eac05c9af54388fbb9c3d7b874d/diff with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/overlay2/d414e8b5f99c94ce4912e0eea3f46e87e65a9eac05c9af54388fbb9c3d7b874d/diff': No such file or directory
 - exit status 1, rootInodeErr: cmd [ionice -c3 nice -n 19 find /rootfs/var/lib/docker/overlay2/d414e8b5f99c94ce4912e0eea3f46e87e65a9eac05c9af54388fbb9c3d7b874d/diff -xdev -printf .] failed. stderr: find: '/rootfs/var/lib/docker/overlay2/d414e8b5f99c94ce4912e0eea3f46e87e65a9eac05c9af54388fbb9c3d7b874d/diff': No such file or directory
; err: exit status 1, extraDiskErr: du command failed on /rootfs/var/lib/docker/containers/c4008fecf45bd33ec3daaa57f92f3f2793da803dbe0488791917c481cbbab649 with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/containers/c4008fecf45bd33ec3daaa57f92f3f2793da803dbe0488791917c481cbbab649': No such file or directory
 - exit status 1
E0108 22:04:53.993943    5161 fsHandler.go:121] failed to collect filesystem stats - rootDiskErr: du command failed on /rootfs/var/lib/docker/overlay2/04b0d38efdae4fafb2a9504ff0d844f788d853f4da3d52da0cf883eb578641c3/diff with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/overlay2/04b0d38efdae4fafb2a9504ff0d844f788d853f4da3d52da0cf883eb578641c3/diff': No such file or directory
 - exit status 1, rootInodeErr: cmd [ionice -c3 nice -n 19 find /rootfs/var/lib/docker/overlay2/04b0d38efdae4fafb2a9504ff0d844f788d853f4da3d52da0cf883eb578641c3/diff -xdev -printf .] failed. stderr: find: '/rootfs/var/lib/docker/overlay2/04b0d38efdae4fafb2a9504ff0d844f788d853f4da3d52da0cf883eb578641c3/diff': No such file or directory
; err: exit status 1, extraDiskErr: du command failed on /rootfs/var/lib/docker/containers/632a79346ed0d27c4040d16cca78204843f66a5dcd7f8228a19b4b68e00d373d with output stdout: , stderr: du: cannot access '/rootfs/var/lib/docker/containers/632a79346ed0d27c4040d16cca78204843f66a5dcd7f8228a19b4b68e00d373d': No such file or directory
 - exit status 1
